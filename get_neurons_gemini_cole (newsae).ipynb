{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anaiskillian/anaconda3/envs/ml_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_dataset' from partially initialized module 'datasets' (most likely due to a circular import) (/Users/anaiskillian/cs2822r/datasets.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjaxtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Float\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformer_lens\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.11/site-packages/transformer_lens/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hook_points\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evals\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.11/site-packages/transformer_lens/hook_points.py:29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhooks\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformer_lens\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Slice, SliceInput\n\u001b[1;32m     32\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLensHandle\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Dataclass that holds information about a PyTorch hook.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.11/site-packages/transformer_lens/utils.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hf_hub_download\n",
      "File \u001b[0;32m~/cs2822r/datasets.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# WMDP at https://huggingface.co/datasets/cais/wmdp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      7\u001b[0m ds_bio \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcais/wmdp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwmdp-bio\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m ds_chem \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcais/wmdp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwmdp-chem\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_dataset' from partially initialized module 'datasets' (most likely due to a circular import) (/Users/anaiskillian/cs2822r/datasets.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "import transformer_lens.utils as utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/cole/.cache/huggingface/token\n",
      "Login successful\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "with open(\"access.tok\", \"r\") as file:\n",
    "    access_token = file.read()\n",
    "    login(token=access_token)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset  \n",
    "import transformer_lens\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "NEG_SET_SIZE = 500\n",
    "POS_SET_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2486da692a407eb1060cdb5a533b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# load gemma model \n",
    "model = HookedSAETransformer.from_pretrained(\"gemma-2-2b\", device = device)\n",
    "\n",
    "num_layer = 10\n",
    "# load sae on res stream of gemma model, plus cfg and sparsity val\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gemma-scope-2b-pt-res\",\n",
    "    sae_id = f\"layer_{num_layer}/width_16k/average_l0_77\",\n",
    "    device = device\n",
    ")\n",
    "# get layers from here: https://jbloomaus.github.io/SAELens/sae_table/#gemma-scope-2b-pt-res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "#neg_dataset = 'dataset/spanish_harmful.csv'\n",
    "neg_dataset = 'dataset/original_harmful.csv'\n",
    "\n",
    "df = pd.read_csv(neg_dataset)\n",
    "\n",
    "columns_as_arrays = [df[col].values for col in df.columns]\n",
    "\n",
    "array_dict = {col: df[col].values for col in df.columns}\n",
    "\n",
    "negative_set = columns_as_arrays[0]\n",
    "negative_set = negative_set[:NEG_SET_SIZE]\n",
    "print(len(negative_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neg2_dataset = 'dataset/harmful_wiki_cleaned.csv'\n",
    "\n",
    "# df = pd.read_csv(neg2_dataset)\n",
    "\n",
    "# columns_as_arrays = [df[col].values for col in df.columns]\n",
    "\n",
    "# array_dict = {col: df[col].values for col in df.columns}\n",
    "\n",
    "# negative_set_2 = columns_as_arrays[0]\n",
    "# negative_set_2 = negative_set_2[30:NEG_SET_SIZE+30]\n",
    "\n",
    "# negative_set_3 = columns_as_arrays[0]\n",
    "# negative_set_3 = negative_set_3[2030:NEG_SET_SIZE+2030]\n",
    "\n",
    "# print(len(negative_set_2))\n",
    "# print(len(negative_set_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "#pos_dataset = 'dataset/alpaca_spanish.json'\n",
    "pos_dataset = 'dataset/alpaca_data.json'\n",
    "\n",
    "positive = pd.read_json(pos_dataset)\n",
    "\n",
    "positive_set = positive['output'].values\n",
    "positive_set = positive_set[:POS_SET_SIZE]\n",
    "print(len(positive_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m cache\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m positive_set:\n\u001b[0;32m---> 19\u001b[0m     _, cache \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache_with_saes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msae\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# get top 15 firing sae neurons\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     vals, inds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(cache[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblocks.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_layer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.hook_resid_post.hook_sae_acts_post\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :], \u001b[38;5;241m15\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sae_lens/analysis/hooked_sae_transformer.py:228\u001b[0m, in \u001b[0;36mHookedSAETransformer.run_with_cache_with_saes\u001b[0;34m(self, saes, reset_saes_end, use_error_term, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around 'run_with_cache' in HookedTransformer.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03mAttaches given SAEs before running the model with cache and then removes them.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    **kwargs: Keyword arguments for the model forward pass\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaes(\n\u001b[1;32m    226\u001b[0m     saes\u001b[38;5;241m=\u001b[39msaes, reset_saes_end\u001b[38;5;241m=\u001b[39mreset_saes_end, use_error_term\u001b[38;5;241m=\u001b[39muse_error_term\n\u001b[1;32m    227\u001b[0m ):\n\u001b[0;32m--> 228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_cache_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_cache_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:642\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_cache\u001b[39m(\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mmodel_args, return_cache_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    627\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m     Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m    635\u001b[0m ]:\n\u001b[1;32m    636\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around `run_with_cache` in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \n\u001b[1;32m    638\u001b[0m \u001b[38;5;124;03m    If return_cache_object is True, this will return an ActivationCache object, with a bunch of\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;124;03m    useful HookedTransformer specific methods, otherwise it will return a dictionary of\u001b[39;00m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;124;03m    activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 642\u001b[0m     out, cache_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    646\u001b[0m         cache \u001b[38;5;241m=\u001b[39m ActivationCache(cache_dict, \u001b[38;5;28mself\u001b[39m, has_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m remove_batch_dim)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformer_lens/hook_points.py:566\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, pos_slice, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m cache_dict, fwd, bwd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    553\u001b[0m     names_filter,\n\u001b[1;32m    554\u001b[0m     incl_bwd,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    557\u001b[0m     pos_slice\u001b[38;5;241m=\u001b[39mpos_slice,\n\u001b[1;32m    558\u001b[0m )\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(\n\u001b[1;32m    561\u001b[0m     fwd_hooks\u001b[38;5;241m=\u001b[39mfwd,\n\u001b[1;32m    562\u001b[0m     bwd_hooks\u001b[38;5;241m=\u001b[39mbwd,\n\u001b[1;32m    563\u001b[0m     reset_hooks_end\u001b[38;5;241m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    564\u001b[0m     clear_contexts\u001b[38;5;241m=\u001b[39mclear_contexts,\n\u001b[1;32m    565\u001b[0m ):\n\u001b[0;32m--> 566\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    568\u001b[0m         model_out\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:560\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    556\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    557\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    558\u001b[0m         )\n\u001b[0;32m--> 560\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformer_lens/components/transformer_block.py:160\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    153\u001b[0m     key_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    154\u001b[0m     value_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    156\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_normalization_before_and_after:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1_post(attn_out)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformer_lens/components/abstract_attention.py:208\u001b[0m, in \u001b[0;36mAbstractAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mpositional_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrotary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    206\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_rot_q(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_rotary(q, kv_cache_pos_offset, attention_mask))\n\u001b[1;32m    207\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_rot_k(\n\u001b[0;32m--> 208\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_rotary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     )  \u001b[38;5;66;03m# keys are cached so no offset\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [torch\u001b[38;5;241m.\u001b[39mfloat32, torch\u001b[38;5;241m.\u001b[39mfloat64]:\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;66;03m# If using 16 bits, increase the precision to avoid numerical instabilities\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/transformer_lens/components/abstract_attention.py:525\u001b[0m, in \u001b[0;36mAbstractAttention.apply_rotary\u001b[0;34m(self, x, past_kv_pos_offset, attention_mask)\u001b[0m\n\u001b[1;32m    522\u001b[0m x_flip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotate_every_two(x_rot)\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 525\u001b[0m     rotary_cos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotary_cos\u001b[49m[\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m, past_kv_pos_offset : past_kv_pos_offset \u001b[38;5;241m+\u001b[39m x_pos, \u001b[38;5;28;01mNone\u001b[39;00m, :\n\u001b[1;32m    527\u001b[0m     ]\n\u001b[1;32m    528\u001b[0m     rotary_sin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_sin[\n\u001b[1;32m    529\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m, past_kv_pos_offset : past_kv_pos_offset \u001b[38;5;241m+\u001b[39m x_pos, \u001b[38;5;28;01mNone\u001b[39;00m, :\n\u001b[1;32m    530\u001b[0m     ]\n\u001b[1;32m    531\u001b[0m     x_rotated \u001b[38;5;241m=\u001b[39m x_rot \u001b[38;5;241m*\u001b[39m rotary_cos \u001b[38;5;241m+\u001b[39m x_flip \u001b[38;5;241m*\u001b[39m rotary_sin\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1675\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1674\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1677\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sae.use_error_term\n",
    "\n",
    "top_neurons_neg = defaultdict(list)\n",
    "top_neurons_pos = defaultdict(list)\n",
    "\n",
    "for example in negative_set:\n",
    "    _, cache = model.run_with_cache_with_saes(example, saes=[sae])\n",
    "\n",
    "    # get top 15 firing sae neurons\n",
    "    vals, inds = torch.topk(cache[f'blocks.{num_layer}.hook_resid_post.hook_sae_acts_post'][0, -1, :], 15)\n",
    "\n",
    "    for datapoint in zip(inds, vals):\n",
    "        top_neurons_neg[int(datapoint[0])].append(datapoint[1].item())\n",
    "    \n",
    "    del cache\n",
    "    \n",
    "\n",
    "for example in positive_set:\n",
    "    _, cache = model.run_with_cache_with_saes(example, saes=[sae])\n",
    "\n",
    "    # get top 15 firing sae neurons\n",
    "    vals, inds = torch.topk(cache[f'blocks.{num_layer}.hook_resid_post.hook_sae_acts_post'][0, -1, :], 15)\n",
    "    for datapoint in zip(inds, vals):\n",
    "        top_neurons_pos[int(datapoint[0])].append(datapoint[1].item())\n",
    "    \n",
    "    del cache\n",
    "\n",
    "print(top_neurons_neg)\n",
    "print(top_neurons_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len: 513. Filtered negative neurons: {15292: [7.964665412902832, 16.30695343017578, 8.415923118591309, 6.3382487297058105, 8.156566619873047, 11.515135765075684], 3379: [7.7817606925964355, 6.007218837738037, 12.98498821258545, 7.247117519378662, 10.880072593688965, 7.22714376449585, 7.082031726837158, 6.972873210906982], 13546: [7.021005153656006, 6.136992931365967, 6.902019023895264, 8.175981521606445, 5.787267208099365, 11.107040405273438, 6.286914348602295, 7.55910062789917, 9.964818954467773, 8.012052536010742, 9.028322219848633, 12.947704315185547, 8.307945251464844, 6.299863338470459, 9.153085708618164], 2404: [8.085074424743652, 12.471602439880371], 8866: [7.907463550567627, 5.492731094360352], 91: [10.679744720458984, 6.5384321212768555, 7.526253700256348, 7.507866859436035, 7.335987091064453, 5.882230758666992, 9.04326057434082, 6.656044006347656], 11992: [8.675148963928223, 11.662571907043457, 6.6029486656188965, 7.13411283493042, 9.865836143493652], 3965: [7.616941928863525, 4.946096897125244, 7.100021839141846, 6.312069416046143, 21.714252471923828, 11.672392845153809, 11.924019813537598], 5734: [7.086487293243408], 10623: [6.679380893707275, 7.109062194824219, 13.365859031677246, 5.516015529632568, 8.074362754821777, 7.968879222869873, 11.226516723632812, 12.724663734436035], 11116: [6.489773273468018], 2471: [38.62544250488281], 14622: [23.24109649658203, 11.709680557250977], 12446: [17.459320068359375, 6.4477081298828125, 6.871805191040039, 6.265778064727783, 14.973542213439941, 9.866021156311035, 9.67960262298584, 8.7904052734375, 19.15839385986328, 7.301965713500977, 8.586338996887207, 11.179405212402344], 1955: [12.213821411132812], 4059: [9.687629699707031], 4956: [9.34788990020752, 13.899797439575195, 12.439867973327637], 4603: [9.031368255615234, 14.855737686157227, 7.586983680725098, 13.181977272033691], 3738: [8.369845390319824], 5525: [7.618550777435303, 7.095721244812012, 7.429974555969238, 6.532369136810303, 6.4822282791137695, 6.3359222412109375, 6.494124412536621, 7.7380805015563965, 8.989274978637695], 13504: [6.06282901763916, 5.958459377288818, 6.988171100616455, 6.590578079223633, 11.479019165039062, 7.898678302764893, 10.560898780822754, 8.990901947021484, 7.517215251922607, 6.587207317352295], 13676: [5.913242816925049, 6.72870397567749], 9866: [5.301824569702148, 7.124000549316406, 6.2623491287231445, 6.331888198852539, 8.100489616394043, 11.02004337310791, 6.7087202072143555], 976: [8.29890251159668], 10872: [8.102522850036621], 5363: [7.240872383117676, 9.686250686645508, 7.237001895904541, 9.2853364944458, 10.841887474060059, 6.235101222991943], 12962: [6.712575435638428, 7.549930572509766], 13321: [16.90323257446289], 16333: [15.027780532836914], 15759: [14.835556030273438, 11.289799690246582, 16.022668838500977], 13304: [13.278793334960938, 12.471985816955566], 26: [12.078985214233398], 12411: [9.854890823364258], 13399: [10.680549621582031, 5.863767147064209, 20.812225341796875, 5.275777816772461, 11.021390914916992], 607: [8.996481895446777, 5.768670558929443, 13.802220344543457], 4565: [6.188896656036377], 2780: [15.363723754882812, 11.438048362731934, 9.940470695495605, 9.830867767333984], 10248: [7.841033935546875], 4460: [6.875383377075195, 22.91494369506836], 10319: [6.019495964050293, 10.438032150268555, 10.438032150268555], 1495: [10.179851531982422, 12.899770736694336, 11.404237747192383, 9.075811386108398, 12.591879844665527, 10.419276237487793, 7.947754859924316, 8.745807647705078, 21.092050552368164, 14.255546569824219, 32.23450469970703, 17.203899383544922, 12.635420799255371, 10.396037101745605, 8.837789535522461, 8.171817779541016, 25.179048538208008, 15.122002601623535, 11.888041496276855, 11.383219718933105, 13.861295700073242, 13.250555038452148, 19.94426918029785, 9.76239013671875, 14.594064712524414, 10.529260635375977, 16.378576278686523, 9.747337341308594, 11.745084762573242, 7.58637809753418, 7.718938827514648, 10.247537612915039, 9.96948528289795, 7.652019500732422], 5458: [9.478351593017578, 14.159634590148926, 11.644800186157227, 12.239997863769531, 10.346782684326172, 9.923110961914062, 10.02116870880127, 7.867501258850098, 6.590826511383057, 14.19829273223877, 5.601275444030762, 7.4731950759887695, 8.54550552368164], 3870: [14.905253410339355, 11.47921371459961], 12505: [12.93415355682373], 15701: [11.787165641784668, 14.549871444702148], 6286: [11.19675350189209], 3394: [6.814104080200195], 6451: [5.886167526245117, 6.893296241760254, 7.035918235778809], 11203: [36.04328536987305], 10468: [15.362756729125977, 5.978523254394531], 1323: [13.528078079223633, 16.46116065979004], 13087: [13.342915534973145], 11148: [11.238134384155273, 11.069421768188477], 4664: [8.02175235748291, 5.842574119567871, 6.471071720123291, 6.825563907623291], 2974: [7.204662322998047, 7.224642753601074, 9.487930297851562, 6.507262706756592], 7241: [6.842164039611816], 4255: [6.5650811195373535], 13474: [11.837285995483398, 7.574217319488525, 6.655710220336914, 28.02274513244629, 9.094768524169922], 5370: [16.357086181640625, 11.605649948120117], 13307: [10.719188690185547], 1530: [10.580830574035645, 9.466179847717285, 5.791808128356934, 7.796960353851318, 8.199803352355957, 7.881917476654053, 9.204901695251465, 5.859673976898193], 13388: [7.970984935760498], 8420: [12.623947143554688, 7.972606182098389, 24.13873291015625, 31.046220779418945, 11.461400985717773, 8.876976013183594, 46.65155029296875, 34.022735595703125, 9.468021392822266, 11.49380874633789, 11.214855194091797], 9223: [6.841630935668945, 6.988554954528809, 5.773185729980469], 189: [11.971571922302246], 16203: [9.00798511505127, 8.13232135772705, 18.017045974731445, 14.789605140686035], 820: [8.054953575134277], 5351: [7.503868579864502, 9.889522552490234], 10606: [6.7930498123168945, 8.7921781539917], 10805: [6.052698135375977, 7.494476318359375], 6638: [34.78670883178711], 13862: [16.19510841369629], 9198: [15.92446231842041, 6.313626766204834], 3069: [14.604876518249512, 9.715304374694824], 6447: [12.853068351745605], 10443: [11.800935745239258], 15794: [10.567665100097656, 8.113356590270996], 12825: [9.532008171081543, 12.61721420288086, 9.621233940124512, 11.008563041687012, 9.582714080810547, 7.581843376159668], 12570: [9.447668075561523], 9780: [8.907493591308594], 10547: [30.393810272216797], 5873: [20.773681640625], 7047: [14.41635513305664, 11.26142406463623, 8.31284236907959, 7.045619964599609, 11.366387367248535], 9020: [11.237104415893555, 13.572917938232422, 22.050010681152344], 11274: [9.381926536560059], 1389: [8.341476440429688], 6231: [8.139365196228027], 11620: [7.99033784866333], 3568: [37.043941497802734], 1648: [35.48149490356445], 5785: [21.784059524536133, 8.071505546569824, 9.258216857910156], 7110: [10.628558158874512], 550: [11.677000999450684], 6163: [10.820039749145508], 3556: [9.389137268066406, 14.866353988647461, 21.151071548461914], 4322: [9.253090858459473], 6292: [5.5714874267578125], 6770: [5.563368320465088, 11.731491088867188, 6.067801475524902, 16.655092239379883, 9.404654502868652, 7.295382022857666, 6.744252681732178, 14.374661445617676, 16.655092239379883, 7.5799078941345215, 17.77158546447754], 10930: [28.596445083618164, 6.266395568847656], 8800: [24.471393585205078, 6.847614288330078, 7.5393452644348145, 7.476149082183838], 13218: [14.487824440002441, 9.304350852966309, 11.441352844238281, 6.794817924499512, 7.079659461975098], 13350: [10.212833404541016], 6948: [7.959174156188965], 9000: [7.5584716796875], 12772: [7.459316253662109], 12042: [10.039196968078613, 5.896380424499512], 14430: [9.711349487304688], 6561: [8.832122802734375], 10400: [8.540847778320312, 7.2319536209106445, 5.345207691192627, 9.103757858276367, 7.61431884765625, 6.707056999206543, 7.096221923828125], 7091: [7.294973373413086, 16.075498580932617], 14220: [31.741439819335938], 859: [19.17270278930664], 10548: [10.412923812866211, 8.385196685791016, 11.37968635559082, 10.405065536499023], 10097: [9.503686904907227], 3945: [9.213953018188477], 14449: [11.311663627624512, 7.950823783874512, 6.49280309677124, 6.1217360496521, 6.520105361938477, 16.138620376586914, 24.83035659790039, 8.155621528625488, 9.046601295471191, 7.381585121154785], 7069: [8.628230094909668, 8.111149787902832, 8.883383750915527, 7.0469770431518555, 8.104494094848633], 15460: [11.521478652954102], 15678: [8.205126762390137, 9.063267707824707], 11: [7.871041297912598, 11.985971450805664, 12.103250503540039, 6.9114789962768555], 13887: [7.4704437255859375, 8.466497421264648, 11.66065502166748], 14118: [11.286226272583008, 10.960322380065918, 18.770727157592773, 30.667264938354492, 11.438365936279297, 22.983705520629883, 6.937379837036133, 13.525965690612793, 10.80714225769043, 8.497050285339355], 14826: [7.9711127281188965, 6.619569778442383, 9.808035850524902], 7222: [5.923618793487549, 7.523437976837158], 12124: [24.461177825927734, 24.41840934753418], 6855: [10.465657234191895, 14.515995979309082], 2811: [8.694653511047363, 20.972366333007812], 3329: [7.69654655456543], 5125: [11.369595527648926, 7.529823303222656, 12.184991836547852, 12.852362632751465, 6.128481864929199], 16174: [6.631636619567871], 9125: [7.848401069641113, 6.561233043670654, 10.958772659301758], 6403: [5.070464611053467, 7.480159282684326], 4662: [34.815757751464844], 3347: [12.886621475219727], 2085: [10.608271598815918], 5653: [9.383323669433594], 1373: [9.29445743560791], 7566: [9.579623222351074, 12.451272964477539, 8.070535659790039, 10.173273086547852, 19.676578521728516, 11.302571296691895, 7.6055498123168945], 10908: [8.872272491455078, 7.6876139640808105, 16.838661193847656], 12065: [8.409590721130371, 6.188396453857422, 9.554718017578125], 4406: [6.316396713256836], 2876: [20.358150482177734, 23.66299057006836, 20.11284828186035], 12442: [14.916631698608398, 13.210216522216797], 7517: [10.105488777160645, 7.030231952667236], 11264: [8.622980117797852], 10894: [8.480189323425293, 12.24183464050293], 5751: [7.940343856811523], 13343: [7.869743347167969, 7.114828109741211], 10726: [7.860753536224365], 14200: [7.6265153884887695, 7.796084403991699], 6015: [6.579743385314941], 4886: [7.410942077636719], 5669: [6.907517433166504, 6.602275848388672], 3297: [6.333793640136719], 6998: [5.9860520362854, 6.622283458709717], 2416: [6.121151447296143, 6.665172576904297], 3849: [26.404199600219727], 2550: [25.780290603637695], 2086: [13.19881820678711], 9905: [11.726204872131348], 15949: [10.407864570617676], 15802: [9.718241691589355], 9157: [9.27355670928955], 15598: [8.762870788574219], 16009: [8.545366287231445, 13.694786071777344, 9.54417610168457], 15568: [8.479935646057129], 15029: [5.806249141693115], 542: [6.652076721191406], 12742: [9.66564655303955], 6632: [6.8698225021362305, 8.665790557861328, 6.026902198791504, 6.769412040710449], 12961: [6.052062034606934], 10078: [7.377807140350342], 13430: [6.492361068725586], 2401: [6.330533027648926, 7.8966827392578125, 6.112052917480469], 3923: [11.883578300476074, 11.883578300476074], 18: [11.330963134765625, 11.330963134765625], 2313: [10.886041641235352, 10.886041641235352], 13379: [10.455137252807617, 12.268522262573242, 10.455137252807617], 14326: [19.928531646728516, 17.568220138549805, 13.953417778015137, 13.729691505432129, 21.25980567932129], 11630: [14.221925735473633, 11.143267631530762, 16.195537567138672, 12.388341903686523, 14.880366325378418, 11.592148780822754], 15268: [12.621651649475098, 11.137558937072754, 22.702455520629883], 3705: [12.095836639404297, 9.74307918548584], 11525: [11.350415229797363, 13.226119041442871], 2192: [10.567621231079102], 15205: [6.89116096496582, 6.049233436584473], 1053: [9.613703727722168], 11566: [8.607746124267578], 6836: [5.8721795082092285], 5982: [17.642187118530273, 6.850647926330566], 15351: [10.816619873046875], 644: [7.565968990325928, 10.609593391418457, 11.346517562866211, 11.609904289245605, 9.103928565979004], 10348: [13.928064346313477], 13746: [10.223248481750488, 11.707000732421875], 620: [9.867910385131836], 5486: [9.628323554992676], 5835: [6.673515796661377, 7.394489765167236], 12957: [20.156408309936523, 8.202378273010254], 9459: [19.732345581054688], 5644: [17.668323516845703, 9.283759117126465], 13339: [17.083633422851562, 9.228622436523438], 3326: [13.699014663696289, 16.99592399597168], 3039: [12.406134605407715], 5409: [12.256604194641113, 6.9692182540893555, 18.35053253173828, 15.159812927246094], 2648: [11.498943328857422, 18.670969009399414, 13.157462120056152], 10236: [22.274494171142578, 11.615482330322266, 15.648736000061035], 12755: [20.50314712524414], 12044: [9.954973220825195], 7203: [9.684624671936035], 6767: [9.583585739135742, 11.95036506652832, 9.349674224853516], 10534: [6.918261528015137, 9.041070938110352], 3233: [6.955155372619629, 6.982555389404297, 10.597893714904785], 2908: [32.17457962036133], 9859: [15.2467679977417], 7862: [12.827978134155273], 13407: [11.465222358703613, 19.812742233276367], 3554: [10.110398292541504], 7726: [24.645463943481445], 4078: [18.492542266845703], 7030: [13.654169082641602, 5.9938645362854], 2691: [11.070470809936523], 11820: [10.375956535339355, 7.928700923919678, 18.950664520263672, 18.388750076293945], 9454: [10.058837890625], 954: [9.54945182800293], 9942: [12.185643196105957, 12.801080703735352, 11.214855194091797], 9089: [7.188025951385498], 10994: [7.428447723388672], 9420: [7.328884124755859], 9787: [6.944908142089844, 6.809832572937012, 9.36083698272705], 14880: [6.50486946105957, 11.511617660522461], 3779: [24.98600959777832, 28.071060180664062, 28.2772159576416], 8889: [21.847402572631836, 19.967388153076172, 18.087722778320312], 5786: [7.158666610717773], 1849: [6.881421089172363], 1402: [10.396405220031738, 6.676412582397461], 14846: [10.178508758544922, 7.505351543426514], 8751: [8.339065551757812, 8.611637115478516], 3066: [7.383487701416016], 9874: [7.043040752410889], 10429: [6.474419116973877], 8317: [7.236422538757324], 7787: [7.5531158447265625], 11353: [21.092042922973633, 32.562644958496094], 15626: [13.73306941986084], 2268: [12.303672790527344, 12.07630729675293], 2861: [11.073371887207031], 418: [9.700898170471191, 11.44805908203125], 8973: [9.640725135803223], 10228: [11.216267585754395, 6.299544811248779], 3895: [10.5364990234375], 5173: [8.750338554382324], 16271: [5.545660495758057], 8781: [7.763288974761963], 4356: [6.629287242889404], 12614: [6.373610496520996, 10.027798652648926], 14988: [6.473123550415039, 5.900918006896973, 7.448236465454102], 7799: [16.399900436401367], 3130: [15.237659454345703, 14.512114524841309, 9.685789108276367], 7266: [13.350326538085938], 2707: [21.22313117980957], 8967: [15.536310195922852], 10603: [13.495641708374023], 10650: [10.117071151733398], 4608: [10.067757606506348], 11098: [9.437640190124512], 2856: [7.722838401794434], 10840: [7.796631813049316], 6172: [17.662973403930664, 14.222747802734375], 11440: [17.443153381347656, 13.898942947387695], 9699: [16.956098556518555, 12.740635871887207], 11833: [15.296364784240723], 13155: [7.407923698425293, 17.451784133911133], 1941: [8.202192306518555, 7.361578464508057], 8701: [7.632166862487793], 9292: [5.847414016723633], 2719: [7.633163928985596], 12922: [6.090816974639893], 15863: [5.643552303314209], 16204: [7.148090839385986], 8110: [6.358834266662598], 5840: [13.559080123901367], 14730: [33.159942626953125], 3726: [14.815810203552246], 4449: [10.756444931030273], 2985: [10.611369132995605], 5454: [9.745640754699707], 5621: [9.189699172973633, 10.052742958068848, 5.893789291381836], 13066: [9.131795883178711, 7.3586602210998535], 3832: [20.426219940185547], 9762: [11.702533721923828], 9944: [11.50692367553711], 8424: [10.659032821655273], 1334: [7.699246406555176], 6394: [5.906525611877441], 14069: [7.346909046173096, 12.77246379852295], 11673: [24.829669952392578], 2781: [13.490482330322266], 15753: [11.252212524414062], 12819: [10.569354057312012], 10749: [10.548970222473145], 14212: [9.79995059967041], 12284: [9.035994529724121], 8765: [6.843367576599121, 8.402881622314453], 14655: [7.03265380859375, 10.038835525512695], 4961: [14.693273544311523], 1678: [14.018872261047363], 10380: [12.926504135131836], 7681: [12.128388404846191], 12137: [11.268864631652832], 9240: [10.19531536102295], 9796: [10.101044654846191], 6137: [16.714807510375977], 14892: [16.630382537841797], 4107: [13.0817289352417], 6809: [12.891244888305664], 15362: [10.870403289794922], 9930: [9.148216247558594], 13476: [8.862709045410156], 2318: [8.723457336425781, 7.872440338134766], 8878: [8.594194412231445], 13795: [8.18444538116455], 4081: [6.441777229309082], 7538: [6.299154281616211], 8676: [6.3270039558410645, 19.054176330566406, 11.610955238342285], 1514: [19.683412551879883], 15092: [11.788147926330566, 19.468029022216797], 7693: [11.111695289611816], 12428: [15.906278610229492], 9882: [10.664721488952637], 7836: [9.814051628112793], 8382: [33.4359016418457, 31.801280975341797], 6893: [10.609724998474121, 11.564323425292969], 6659: [9.192241668701172, 7.809589862823486], 7948: [14.619714736938477], 5085: [13.222217559814453], 7377: [11.654658317565918], 1235: [11.238922119140625], 9498: [11.186668395996094, 10.846564292907715], 2400: [9.927804946899414], 8531: [9.772367477416992], 12484: [9.552699089050293], 7271: [8.417579650878906], 4872: [23.057857513427734], 14253: [16.556930541992188, 10.833476066589355], 1266: [14.151346206665039], 8166: [14.075379371643066], 7617: [10.094643592834473, 6.522109031677246], 5662: [9.986062049865723], 9350: [9.740091323852539], 30: [8.020635604858398, 29.673736572265625], 1689: [7.855393409729004], 4782: [9.872940063476562], 13653: [6.975576400756836, 7.843498229980469], 5856: [8.763365745544434], 7144: [23.9982967376709], 3287: [12.267500877380371], 6631: [10.858675956726074], 5070: [10.455059051513672], 4583: [6.697722434997559], 16044: [5.556121826171875], 13249: [5.189185619354248, 11.491557121276855], 1264: [11.684722900390625], 10398: [19.441953659057617], 2518: [8.336230278015137], 4877: [7.736320972442627], 9916: [7.599081516265869], 6974: [27.401878356933594], 798: [14.673486709594727], 8515: [13.491074562072754], 2050: [13.275815963745117], 6564: [13.173751831054688], 152: [9.773344993591309], 3918: [9.433855056762695], 5833: [9.428571701049805, 5.698967933654785], 11766: [8.616390228271484], 9609: [7.119616508483887], 12355: [14.153885841369629, 14.72095012664795], 13556: [11.463249206542969], 2752: [11.070631980895996], 14339: [10.737615585327148], 2354: [10.419676780700684], 4327: [14.95864486694336, 8.36284065246582], 4938: [12.294044494628906], 11052: [11.859333038330078], 2924: [10.763445854187012], 3866: [24.267955780029297], 5920: [8.639070510864258], 14304: [11.502470016479492], 6340: [10.937504768371582], 5578: [7.3419718742370605], 9275: [14.374418258666992], 1193: [6.294642448425293], 15648: [7.738186359405518], 13601: [10.829139709472656], 1823: [8.431965827941895], 2870: [6.457505702972412], 1756: [12.690093040466309], 11144: [23.07870864868164], 11536: [19.73076820373535], 15111: [16.024354934692383], 1813: [9.117399215698242], 10806: [7.568193435668945], 14208: [7.413095474243164], 2732: [7.080864906311035], 7613: [9.547211647033691], 11600: [24.164094924926758], 14394: [23.70315933227539], 2990: [12.02091121673584], 14928: [9.868319511413574], 6837: [8.68433952331543], 5322: [6.897898197174072], 6936: [6.305174827575684], 7988: [6.397317409515381], 3969: [26.45317840576172], 9564: [18.637941360473633], 12684: [17.81882095336914], 7961: [14.167808532714844], 15935: [11.942024230957031], 3715: [11.608346939086914], 10735: [10.793416023254395], 5037: [10.769100189208984], 7944: [28.661096572875977], 8125: [10.316664695739746], 3943: [8.87067985534668], 4235: [7.648834228515625], 11960: [9.800041198730469], 14632: [10.185949325561523, 10.109040260314941], 1640: [9.340346336364746], 9259: [8.169916152954102], 12898: [7.760173320770264], 9386: [7.035792350769043], 3887: [6.721861839294434, 5.733016014099121], 10676: [6.008763313293457], 14514: [5.919366359710693], 13255: [12.725618362426758], 10430: [16.850221633911133], 407: [7.779247283935547], 13582: [7.32191801071167], 5381: [18.648818969726562], 14924: [14.757266998291016], 2015: [12.657212257385254], 14506: [12.58944320678711], 14433: [11.4964017868042], 7017: [10.255611419677734], 8205: [22.22906494140625], 2515: [18.215850830078125], 1598: [9.251839637756348], 16200: [8.943470001220703, 11.934309005737305], 2754: [8.29523754119873], 13741: [7.893383979797363], 2756: [7.622688293457031], 3581: [6.179888725280762], 7390: [10.994489669799805], 16100: [7.227034568786621, 8.478418350219727], 14666: [6.67446231842041], 15098: [19.968841552734375, 11.195871353149414], 5912: [18.824138641357422, 6.297356128692627, 10.62330150604248], 3507: [10.334158897399902], 13980: [9.250886917114258, 8.305219650268555], 5588: [8.853419303894043], 228: [8.641521453857422], 11094: [8.555841445922852], 10203: [7.830835819244385], 15792: [7.577243328094482, 6.63577127456665], 4227: [6.40371036529541], 5111: [6.5106587409973145], 8499: [5.962088108062744], 12051: [8.194281578063965], 8460: [38.49020767211914], 2136: [9.340632438659668, 10.849020957946777], 3560: [9.086880683898926], 11803: [7.6105546951293945], 1636: [23.641515731811523], 9666: [18.961650848388672], 955: [17.05980110168457], 12308: [14.389626502990723], 11708: [10.726726531982422], 1092: [9.379988670349121], 11837: [9.01133918762207], 3510: [8.468743324279785], 5449: [7.0049591064453125], 12665: [6.8216705322265625], 9073: [8.154757499694824], 14063: [26.024763107299805], 2595: [21.299474716186523], 10581: [16.80152130126953], 12086: [8.76212215423584], 14941: [7.163119792938232], 10807: [6.303053855895996], 2218: [5.890646934509277], 5355: [12.766338348388672], 12823: [12.609054565429688, 11.132627487182617], 10082: [9.999406814575195], 14417: [8.381895065307617], 15310: [7.9063873291015625, 8.030821800231934], 8027: [6.611349105834961], 6532: [28.667980194091797], 7765: [19.539093017578125], 12741: [13.344392776489258], 4191: [12.054239273071289], 7880: [8.60216236114502], 1781: [7.204043388366699], 13115: [6.7738938331604], 513: [10.109487533569336], 4714: [10.089309692382812], 11695: [22.44240951538086], 6556: [8.481573104858398], 6076: [7.942646026611328], 5829: [7.050298690795898], 6926: [22.342933654785156], 2442: [15.757440567016602], 9151: [9.714576721191406], 16293: [7.76645565032959], 11242: [7.715145587921143]}\n",
      "Len: 666. Filtered positive neurons: {3288: [18.667478561401367, 18.462013244628906, 10.791808128356934, 8.45787239074707, 12.276546478271484, 13.585144996643066, 10.386338233947754, 6.9524736404418945, 5.8918046951293945, 7.773276329040527, 18.20948600769043, 28.556209564208984, 14.381340980529785, 13.874963760375977, 12.514699935913086, 17.30168914794922, 14.77572250366211, 8.631387710571289, 16.07554054260254, 19.189916610717773, 9.275469779968262, 9.495835304260254, 11.864448547363281, 17.26776885986328, 11.555049896240234, 16.855554580688477, 11.064334869384766, 11.21158504486084, 10.464569091796875, 12.866958618164062, 10.785463333129883, 5.5386481285095215, 13.222697257995605, 8.789826393127441, 8.934547424316406], 7081: [17.13899040222168, 8.607427597045898, 16.89360237121582, 9.709430694580078, 9.88575553894043, 18.094364166259766, 18.376710891723633, 17.850419998168945, 18.908397674560547, 13.788114547729492, 11.51626968383789, 11.255903244018555], 1909: [9.669475555419922], 2652: [7.466577529907227, 5.7684736251831055, 6.9877824783325195, 5.699833869934082, 7.7512922286987305, 11.729429244995117, 9.154284477233887, 6.857558250427246, 10.174407958984375, 9.988065719604492, 10.090600967407227, 8.460404396057129, 8.174912452697754, 6.175778388977051], 14062: [6.9941086769104, 15.366476058959961], 14810: [6.775144100189209, 9.117748260498047, 6.8862481117248535, 7.008885860443115], 10247: [6.710412979125977, 7.510305404663086, 8.758173942565918, 7.111148834228516, 9.931861877441406, 8.155924797058105, 7.397735595703125, 11.56769847869873, 7.077728271484375, 10.391512870788574, 12.299717903137207, 8.805553436279297], 16067: [7.749514579772949], 12277: [7.227126598358154], 16116: [6.790341854095459, 7.381636142730713, 11.956607818603516, 5.765300273895264, 9.642204284667969, 9.35549545288086, 7.064645290374756, 6.564660549163818, 6.182677745819092, 13.369560241699219, 10.245275497436523, 16.23551368713379, 8.380985260009766, 7.27933931350708, 4.786951541900635], 5807: [6.625948905944824, 9.28793716430664, 7.1056318283081055, 12.246243476867676, 7.531380653381348, 9.118005752563477], 5325: [9.586183547973633, 6.7001051902771, 8.231786727905273, 7.325084209442139, 15.334707260131836, 6.748227596282959, 6.031066417694092, 5.627333164215088, 13.717615127563477, 9.168296813964844, 5.999293804168701, 6.341699123382568], 16225: [8.908103942871094], 13270: [8.828170776367188, 18.450145721435547, 14.198355674743652, 12.945198059082031, 17.024499893188477, 16.756467819213867, 18.69927215576172, 18.9892635345459, 8.04685115814209, 14.777506828308105, 15.143509864807129, 18.634511947631836, 15.610791206359863, 7.919759750366211, 9.047179222106934, 12.735217094421387, 13.827637672424316, 16.570083618164062, 13.875454902648926, 9.038101196289062, 8.47652816772461, 10.785812377929688, 10.346802711486816, 7.520580291748047, 15.41515064239502, 13.651097297668457, 12.30819034576416, 11.060892105102539, 9.490689277648926, 15.156096458435059, 16.257583618164062, 14.473395347595215, 15.306485176086426, 18.858074188232422, 10.14930534362793, 16.615203857421875, 14.071843147277832, 16.599411010742188, 8.879596710205078, 13.208471298217773, 14.128439903259277, 13.503279685974121, 17.093971252441406, 11.783764839172363, 13.918599128723145, 13.163920402526855, 8.800677299499512, 19.111921310424805, 8.953020095825195, 11.657937049865723, 16.2038516998291, 13.9376859664917], 11141: [6.033768653869629, 6.737673759460449, 10.28646183013916, 8.570624351501465, 8.399383544921875, 9.095296859741211, 6.606201171875, 7.62831974029541, 6.912927627563477, 8.325817108154297, 8.047536849975586, 5.45365571975708, 5.838542938232422, 7.831493377685547, 5.668960094451904], 8591: [5.609358787536621], 6493: [10.089618682861328, 13.751890182495117, 9.073058128356934, 9.097809791564941, 5.992830276489258, 7.747857093811035, 8.049332618713379, 8.029210090637207, 9.042737007141113, 7.592892646789551, 11.057406425476074, 7.122727394104004, 8.696503639221191, 9.129000663757324, 5.950417995452881, 7.162569046020508, 7.2787933349609375, 13.60615062713623, 10.188681602478027, 8.195777893066406, 20.045909881591797, 7.064885139465332, 6.2418718338012695], 15601: [8.657003402709961, 7.421446323394775, 6.7499613761901855], 43: [8.198759078979492, 7.064419746398926, 8.594205856323242, 8.682003021240234, 7.465238571166992, 8.108139991760254, 8.369291305541992, 10.735761642456055, 6.886922836303711, 7.853663444519043, 6.592644691467285, 7.323015213012695, 7.000667572021484, 8.840332984924316, 8.84196662902832, 9.385092735290527, 10.33029842376709, 12.031598091125488, 5.828287124633789, 7.88361930847168], 4543: [10.9359769821167, 11.353846549987793], 8504: [10.354072570800781, 7.794666767120361], 15185: [9.457283973693848, 21.710651397705078], 8387: [9.303422927856445], 8810: [9.204327583312988], 12616: [9.881926536560059, 10.80440616607666, 12.771770477294922, 7.620977878570557, 5.80415678024292], 4092: [9.100598335266113, 8.377279281616211], 2308: [7.429306983947754], 11256: [7.356356620788574], 1476: [7.207345962524414], 11998: [6.684667587280273], 8014: [5.769023895263672, 8.363494873046875, 6.251735687255859], 8388: [11.264446258544922], 9095: [9.893891334533691], 9610: [8.822484970092773, 10.453668594360352], 13766: [7.526369094848633, 7.027761459350586], 636: [7.163506984710693, 9.498689651489258, 6.859512805938721, 6.169505596160889], 12814: [7.037193298339844], 9706: [6.909572124481201], 5997: [6.330495834350586, 16.319581985473633], 6025: [5.99273681640625], 14930: [17.56692886352539], 11018: [8.997886657714844], 14285: [7.768348693847656], 6160: [7.268894672393799], 12620: [6.489543437957764], 15040: [8.56606388092041], 10198: [7.6226983070373535], 12917: [7.582330226898193], 14609: [7.188216209411621], 1428: [6.567346572875977], 16064: [19.99085235595703, 21.744632720947266], 2366: [10.247774124145508, 6.544826030731201, 8.597519874572754, 10.02558422088623, 5.771565914154053, 8.182601928710938, 7.797855854034424], 1622: [9.248700141906738], 512: [8.988434791564941], 15084: [10.294907569885254, 10.575190544128418], 8544: [7.656078815460205, 7.944140434265137, 9.289798736572266], 11157: [6.9171929359436035, 5.930197715759277, 6.011544227600098, 7.933935642242432, 8.174882888793945, 14.776422500610352, 5.396434783935547, 6.072404861450195, 8.340470314025879, 7.991302967071533, 12.200737953186035], 13201: [15.346693992614746], 9291: [14.023347854614258], 10556: [12.260941505432129], 7328: [11.283089637756348], 7639: [8.521895408630371], 16340: [7.745382308959961, 20.57203483581543, 8.184152603149414, 18.10481071472168, 8.903380393981934, 24.348979949951172, 15.506284713745117, 13.64956283569336, 20.365060806274414, 11.250923156738281, 9.403777122497559, 14.143423080444336, 11.750967979431152, 6.393862724304199, 12.653597831726074, 15.74819564819336, 9.82779312133789, 10.261177062988281, 13.926273345947266, 10.458863258361816, 11.294088363647461, 5.578548431396484, 7.961012840270996, 16.99260711669922, 8.401201248168945, 25.636966705322266, 8.023104667663574, 6.729382514953613, 7.073535919189453, 19.384809494018555, 12.720595359802246, 11.973182678222656], 6777: [6.990871906280518, 6.066063404083252, 7.786182880401611], 4642: [42.90320587158203], 5608: [18.749313354492188, 7.762795448303223, 7.109277248382568], 13727: [16.208505630493164], 7915: [13.898648262023926, 10.470602989196777, 11.544411659240723], 4070: [11.292903900146484], 7019: [10.517605781555176], 1081: [9.803109169006348, 7.983933448791504], 42: [9.774113655090332, 7.014007091522217], 7894: [8.55893611907959], 6085: [7.944819927215576, 11.052231788635254], 3053: [7.839719295501709], 11247: [16.396743774414062, 10.738973617553711, 8.79563045501709, 9.49305248260498, 12.245933532714844, 6.186902046203613, 9.80481243133545, 7.4607930183410645, 17.34605598449707, 6.733616352081299, 7.396029949188232, 11.119940757751465, 18.80496597290039, 6.418872833251953, 19.096757888793945, 9.9200439453125], 13004: [7.34686279296875, 8.553757667541504, 7.867067337036133], 13991: [7.307534694671631, 7.399336338043213, 6.975317001342773, 6.005790710449219, 6.120572566986084, 6.15550422668457, 7.033752918243408, 8.304361343383789, 5.793478488922119, 6.426962852478027, 8.035666465759277, 7.324151515960693, 6.9297776222229, 8.970922470092773, 7.699707508087158, 6.813866138458252, 9.944273948669434, 7.37634801864624, 8.646388053894043, 6.289069652557373, 5.237726211547852, 7.7005486488342285], 3932: [6.610304355621338], 6870: [5.637639999389648, 10.452969551086426, 9.715777397155762, 6.320241451263428, 13.168327331542969, 6.29218053817749, 6.125497817993164, 5.501155376434326, 10.08198070526123, 8.267732620239258, 9.381643295288086, 10.183439254760742], 6139: [19.977346420288086, 9.465150833129883], 1739: [16.111846923828125, 25.327510833740234, 28.29510498046875], 8414: [13.2517728805542], 11084: [12.73347282409668], 2583: [12.02914810180664], 7140: [11.974007606506348, 7.5211076736450195], 2542: [11.817139625549316], 4683: [11.610210418701172, 22.859039306640625, 19.04883575439453], 5382: [9.824994087219238], 7028: [9.34341812133789, 8.992996215820312, 7.4587883949279785, 14.298223495483398, 8.921979904174805, 5.747156143188477, 9.48644733428955], 12506: [7.956745624542236, 10.333836555480957, 10.870007514953613], 12062: [6.6015496253967285], 2043: [11.830735206604004], 3176: [9.432018280029297], 10618: [7.162689208984375], 11556: [6.985531330108643], 7129: [8.875921249389648, 11.008413314819336, 8.256169319152832, 5.992859840393066, 8.511030197143555, 8.381308555603027, 5.429102420806885, 6.71661901473999, 7.55310583114624, 8.828630447387695, 7.853387355804443, 6.859138011932373, 6.470860958099365, 8.86081600189209, 6.992178440093994, 6.509235858917236], 14265: [8.329297065734863], 869: [6.772290229797363, 6.721077919006348, 7.893524169921875, 7.546139717102051, 6.548340797424316], 8346: [6.381203651428223], 14590: [5.789248466491699], 4586: [13.828916549682617], 4161: [12.069074630737305], 11034: [12.067731857299805, 9.244080543518066, 16.042970657348633, 30.787782669067383, 7.10667085647583, 23.66433334350586], 7682: [9.855001449584961], 7118: [7.168670177459717, 10.755247116088867], 2059: [13.384129524230957, 7.269327163696289, 6.374079704284668, 6.79840087890625, 6.148375511169434, 10.570621490478516], 5683: [7.80423641204834, 9.043425559997559, 9.434229850769043, 7.342084884643555, 5.957456588745117], 6885: [6.407780170440674], 16075: [6.228035926818848, 8.182578086853027, 8.600269317626953, 7.2425994873046875, 6.6553778648376465, 6.563836574554443, 7.659963607788086, 7.265253067016602], 12740: [26.054536819458008, 8.240190505981445, 19.2734432220459, 8.51254940032959], 7012: [18.00020980834961, 7.095034599304199, 11.11934757232666], 6130: [8.058838844299316, 11.51756763458252, 9.144758224487305, 10.573201179504395, 9.325433731079102, 5.966470241546631, 5.7850141525268555, 9.723701477050781, 7.666499614715576], 901: [6.406519412994385, 7.397624492645264], 9365: [6.272937297821045], 6141: [9.767169952392578], 11865: [8.71518611907959, 9.533205032348633, 10.238261222839355, 9.316786766052246, 10.391820907592773, 10.617524147033691], 14421: [5.699499607086182, 10.94713020324707, 6.736880302429199], 14798: [5.5699286460876465], 3177: [5.478426933288574, 8.771961212158203, 9.713525772094727, 8.870115280151367, 9.36918830871582, 6.571753025054932, 7.274899959564209], 8444: [9.020732879638672], 10155: [7.383376121520996], 13881: [6.436594486236572, 6.978794574737549, 6.971621036529541, 7.765678882598877], 4263: [13.673073768615723], 12156: [9.415502548217773], 9917: [8.575105667114258], 14680: [7.7266035079956055], 8686: [7.142054557800293, 7.9474382400512695], 10988: [14.611037254333496, 6.555244445800781], 9113: [14.239558219909668, 11.853899002075195], 1385: [11.890636444091797, 11.539850234985352, 10.584423065185547, 9.297758102416992, 13.672282218933105], 4171: [8.629647254943848], 3596: [8.153894424438477], 7607: [9.057839393615723], 15504: [8.31008529663086, 9.345739364624023], 9592: [7.241541862487793], 2096: [7.280517578125, 6.38630485534668, 6.974639892578125], 15232: [7.199328899383545, 7.695476055145264, 6.913655757904053, 8.169116020202637, 9.52998161315918], 12880: [17.965076446533203, 12.910083770751953], 11040: [15.601438522338867, 12.568300247192383], 15835: [14.82706069946289], 3975: [10.382098197937012, 6.724893093109131, 11.052569389343262], 8120: [10.187599182128906, 11.192679405212402], 3649: [9.280660629272461, 8.375831604003906], 5386: [8.158814430236816], 4852: [7.79229736328125, 9.002047538757324], 15399: [7.6751837730407715], 14665: [7.309681415557861, 14.150482177734375, 8.090349197387695, 13.053024291992188], 6386: [6.564439296722412], 3867: [6.109480857849121], 8310: [8.088489532470703], 10405: [7.489635467529297, 13.563375473022461], 1265: [5.299051761627197], 10391: [22.55047607421875], 13768: [19.521465301513672], 9463: [19.394027709960938], 1348: [16.998441696166992, 10.136655807495117, 6.931107521057129], 8192: [14.387249946594238], 12631: [14.146195411682129], 13000: [13.056973457336426], 3263: [12.040563583374023], 8861: [11.59205150604248], 4522: [11.159431457519531], 14674: [12.367804527282715, 9.464363098144531], 1059: [8.255349159240723], 10087: [6.311982154846191, 8.860209465026855], 4809: [5.915132522583008, 6.0794477462768555], 13738: [5.773608207702637], 7702: [8.744962692260742, 10.795248031616211], 4791: [7.132217884063721, 13.244492530822754, 11.81438159942627, 8.60735034942627], 1360: [6.983136177062988, 11.678112030029297], 7841: [6.300988674163818], 6152: [6.132709503173828], 12629: [12.128246307373047, 6.826408386230469, 10.575689315795898, 6.43326473236084, 10.908021926879883], 1919: [6.677315711975098], 7710: [6.353580951690674, 6.785284519195557], 11727: [13.310423851013184], 6496: [9.082128524780273], 10816: [7.4156060218811035, 10.74277114868164], 10969: [6.59815788269043, 9.794699668884277, 6.310711860656738], 2109: [6.386298179626465, 9.222282409667969], 15237: [16.915666580200195], 12936: [7.851444244384766], 9321: [7.235759258270264], 1425: [29.63323974609375, 36.73111343383789], 8234: [20.86464500427246, 26.342601776123047], 7210: [18.00068473815918], 7479: [15.125040054321289], 6627: [9.175603866577148], 1978: [8.851483345031738], 2288: [8.834935188293457], 1261: [8.24195384979248], 15502: [8.231379508972168, 17.31257438659668], 14542: [8.037531852722168], 13192: [6.700508117675781], 15039: [5.990009307861328], 14903: [37.82257080078125], 12619: [23.882362365722656, 11.214083671569824], 14792: [20.090723037719727], 6542: [18.587995529174805], 11488: [16.9013729095459], 6209: [15.142374038696289], 7977: [15.10189437866211], 8111: [14.60328483581543, 5.262674808502197, 16.056495666503906], 12032: [12.887386322021484, 8.853408813476562, 10.697728157043457, 11.660383224487305], 2672: [11.538230895996094], 11551: [8.438802719116211], 3653: [8.19442081451416], 8093: [36.75843811035156, 9.029585838317871, 19.421009063720703], 10931: [30.80995750427246, 25.984373092651367, 14.584389686584473, 24.65696907043457, 31.630102157592773, 10.222997665405273, 6.958569049835205, 11.98450756072998], 8991: [26.678638458251953, 27.790122985839844, 13.957032203674316], 11336: [21.425466537475586, 10.901602745056152], 8457: [17.84837532043457, 11.198030471801758], 3078: [12.249686241149902], 13898: [9.962645530700684], 9547: [9.577203750610352, 10.392943382263184, 6.59806489944458], 2035: [9.192607879638672], 11444: [8.73010540008545, 8.710712432861328], 9148: [7.71903133392334, 11.013490676879883, 13.049793243408203, 11.038466453552246, 14.65223503112793, 9.552080154418945], 11929: [7.2697649002075195], 15282: [5.80892276763916, 6.053614616394043], 12794: [7.713062763214111], 2293: [7.577184200286865], 6304: [5.799394607543945, 7.201745986938477], 6780: [5.713325500488281], 14038: [10.624982833862305], 423: [10.422454833984375], 3217: [10.253767967224121, 6.18946647644043], 1892: [9.480999946594238, 13.555076599121094], 9899: [9.046049118041992, 7.639430522918701, 7.373079776763916, 11.299365997314453, 11.786418914794922, 10.18980598449707], 10243: [6.933789253234863, 9.02073860168457, 26.334766387939453], 7544: [11.476593017578125], 13222: [7.147478103637695], 13751: [6.322821617126465], 11463: [7.571018218994141], 15692: [6.940521717071533], 8885: [9.391823768615723], 15484: [8.468605041503906], 12600: [8.344204902648926], 9898: [7.309961795806885], 193: [6.408865451812744, 6.6770148277282715, 6.043875694274902], 7650: [14.273214340209961], 658: [10.546529769897461], 13743: [7.290307998657227, 10.061812400817871], 5427: [10.906209945678711, 8.481523513793945], 13663: [6.458054065704346], 10503: [6.241828918457031, 13.783703804016113, 7.118346691131592, 16.964778900146484, 6.576911449432373, 8.230423927307129], 12161: [5.355254173278809], 2211: [5.2860212326049805, 7.915020942687988], 15476: [14.63336181640625], 10265: [10.294180870056152], 9824: [7.624561786651611, 5.880423069000244, 24.37380599975586, 26.82736587524414, 14.284427642822266, 25.199378967285156, 18.90642547607422, 13.269994735717773, 12.149707794189453, 11.677740097045898], 3094: [7.4675211906433105], 8464: [6.785610198974609], 11328: [12.628811836242676], 11869: [9.525618553161621], 1236: [6.713039398193359, 6.703951358795166], 10517: [11.779431343078613, 17.487672805786133, 6.444425582885742, 9.552090644836426], 16070: [8.052583694458008], 14856: [13.649321556091309], 4537: [7.229029178619385], 13401: [28.114973068237305, 29.85630989074707, 31.792356491088867, 25.387975692749023, 24.524059295654297, 28.4837703704834], 2766: [15.528141975402832], 716: [12.996981620788574, 14.44670295715332, 6.779409408569336, 7.930887222290039, 12.058334350585938, 9.598167419433594, 26.23182487487793, 6.018181324005127], 2306: [6.677746772766113], 12792: [9.067819595336914], 13084: [7.327813625335693], 13475: [7.201234340667725], 3696: [16.961055755615234, 26.739286422729492, 12.012014389038086, 14.03862190246582, 17.764291763305664], 1255: [9.76645278930664], 4983: [8.938990592956543], 2965: [7.844681262969971, 7.2830071449279785, 14.572185516357422, 7.618892192840576], 9766: [9.155564308166504], 15336: [8.18995475769043, 6.082535266876221], 11891: [6.079422473907471, 5.773961544036865, 7.2928338050842285], 15770: [5.935671806335449, 7.307278156280518], 4111: [5.870518207550049], 13020: [5.604207992553711], 2116: [5.522758960723877], 12513: [9.167137145996094], 14617: [8.912874221801758, 8.386089324951172], 5675: [8.331806182861328], 6937: [7.690733909606934], 12049: [7.516566753387451], 9023: [5.489290714263916], 7380: [10.424921035766602], 3459: [8.179837226867676], 4704: [6.676625728607178, 6.642451763153076], 3695: [6.3413920402526855], 9299: [6.107152462005615], 6640: [6.097103595733643], 3736: [203.8424072265625, 188.1835479736328], 14372: [65.7200698852539, 75.5732650756836], 15638: [41.96520233154297, 38.923194885253906], 381: [39.23823928833008, 38.604244232177734], 10115: [28.163246154785156, 24.404125213623047, 17.405746459960938], 9744: [8.703779220581055, 6.935312747955322], 0: [0.0, 0.0], 1: [0.0, 0.0], 2: [0.0, 0.0], 3: [0.0, 0.0], 4: [0.0, 0.0], 6: [0.0], 2753: [9.241042137145996], 1969: [7.315929412841797], 11057: [7.658603191375732], 10508: [5.505027770996094], 2969: [5.2697672843933105], 1582: [5.766501426696777], 12730: [6.073793888092041], 5403: [5.960051536560059], 12458: [20.890668869018555], 1234: [16.362924575805664], 15136: [16.335235595703125, 21.706748962402344, 16.594268798828125], 7145: [14.274262428283691], 12316: [13.167973518371582], 15469: [11.388033866882324], 11953: [9.930712699890137, 9.043222427368164, 12.386991500854492], 10105: [8.939932823181152], 11407: [8.737760543823242], 6850: [7.636010646820068], 510: [7.412257671356201], 11000: [7.109230041503906], 4291: [7.668606758117676], 5074: [7.572844505310059], 4024: [9.09958267211914], 10785: [7.207655906677246], 14216: [5.786647796630859], 13747: [10.953330039978027], 5854: [9.407407760620117], 2588: [9.39610481262207, 9.086843490600586], 12261: [9.349047660827637], 12653: [6.705099582672119], 7814: [5.990576267242432], 10431: [18.674373626708984], 15620: [17.084644317626953], 16132: [15.588080406188965, 5.269492149353027], 1945: [14.228353500366211], 2330: [13.609016418457031], 5099: [12.916558265686035], 15507: [12.729551315307617], 11426: [12.073476791381836], 281: [11.681060791015625], 12909: [11.675965309143066], 6389: [10.991762161254883], 2552: [8.583951950073242], 9567: [7.966311931610107], 6125: [7.4492597579956055], 15312: [9.261818885803223], 9936: [6.0421462059021], 10821: [16.116758346557617], 13093: [14.17969799041748, 10.301931381225586], 12103: [9.896416664123535], 15044: [9.733834266662598, 9.267097473144531], 4027: [8.91853141784668, 6.789588928222656], 9127: [5.9617600440979], 6901: [8.69190502166748, 9.780455589294434], 14271: [8.665349006652832], 10411: [6.290525436401367], 15862: [13.99925708770752], 11716: [12.879783630371094, 10.228582382202148], 10972: [10.429368019104004], 4611: [9.141077041625977], 47: [9.045807838439941], 921: [8.973039627075195], 10798: [43.68315887451172], 9371: [19.851314544677734], 32: [18.58765983581543, 9.737068176269531], 16023: [17.928302764892578], 15922: [13.963662147521973], 14747: [13.733330726623535], 9453: [13.097511291503906], 6903: [11.776729583740234], 12377: [11.5066499710083], 15921: [10.839554786682129], 13996: [10.230472564697266], 4643: [9.233405113220215], 14114: [7.326059818267822], 6875: [6.3327531814575195, 8.937190055847168], 15506: [5.650909900665283], 2721: [6.713348388671875], 13809: [5.867364883422852], 13975: [8.186368942260742], 9956: [6.797900199890137], 2525: [15.083544731140137], 3566: [5.958376884460449], 12787: [8.139265060424805], 1188: [6.834900856018066, 7.91007137298584, 9.878525733947754, 15.281170845031738, 10.19568920135498, 8.061058044433594, 8.876931190490723], 15663: [6.382580757141113], 5155: [5.883705139160156], 6490: [9.333671569824219], 7189: [7.57174015045166], 15510: [7.3252105712890625], 15938: [7.250949382781982], 6287: [5.682095527648926], 14661: [10.008286476135254, 10.973124504089355], 224: [6.65568733215332], 6444: [9.890442848205566], 6539: [8.600098609924316, 8.433860778808594], 1834: [7.800065994262695], 4187: [7.572315216064453, 7.621100902557373, 7.579418182373047], 7348: [6.627679824829102], 5907: [6.603744983673096], 5230: [10.841886520385742], 11051: [6.2868123054504395], 6887: [5.884912967681885, 7.404046535491943], 11565: [8.514337539672852], 13647: [7.703973770141602], 11387: [7.55183219909668], 16129: [7.51875638961792], 11777: [5.574732303619385], 3495: [5.561525344848633], 3952: [5.449692249298096], 9835: [11.993147850036621], 1019: [8.950241088867188, 12.858606338500977], 700: [8.814193725585938], 16261: [7.75491189956665, 8.277239799499512, 15.182896614074707], 5981: [6.985345840454102], 7330: [6.305352687835693], 9760: [5.87697172164917], 7519: [10.821037292480469], 2032: [10.495991706848145], 13960: [8.421378135681152], 4358: [6.8810529708862305], 5456: [11.152559280395508], 4119: [11.396488189697266], 13054: [11.082763671875, 9.01986312866211, 8.046205520629883, 5.772859573364258, 5.754153251647949], 2279: [10.013631820678711], 1535: [7.038799285888672], 10689: [9.46811294555664], 15449: [9.693740844726562], 12226: [6.583174705505371], 156: [6.150077819824219, 7.789759159088135], 11348: [8.050808906555176], 10923: [9.039874076843262], 3822: [8.21763801574707], 601: [7.196558952331543], 14769: [15.175721168518066], 1851: [11.759626388549805], 2004: [10.415909767150879], 14448: [9.683821678161621], 655: [8.98345947265625], 13857: [11.745138168334961], 8946: [10.287463188171387, 8.750041961669922], 15674: [9.467397689819336], 15266: [8.031754493713379], 2562: [19.55923080444336], 8898: [9.738821029663086], 13319: [8.785257339477539], 4202: [7.126095294952393], 13978: [7.114978790283203], 12983: [6.438453674316406], 8381: [5.807302474975586], 10100: [7.267555236816406], 9969: [6.680507659912109, 6.118662357330322, 6.843525409698486], 7943: [10.505637168884277], 8886: [19.285921096801758], 2349: [8.272242546081543], 2961: [6.9545512199401855], 7397: [7.840319633483887], 7088: [11.89856243133545], 15781: [9.400796890258789], 1942: [8.998619079589844], 11287: [11.323370933532715], 11149: [7.6553449630737305], 1620: [7.456532955169678], 13226: [9.155885696411133, 6.684680938720703], 2007: [7.761495590209961], 11138: [10.92042064666748], 7869: [9.823823928833008], 13694: [6.151428699493408], 4849: [22.544551849365234], 634: [12.280994415283203], 881: [8.090625762939453], 5740: [7.250461578369141], 5881: [5.885763645172119], 6714: [13.84533405303955, 16.718788146972656], 176: [12.000570297241211, 12.124025344848633, 11.724308013916016], 6795: [10.16360855102539, 8.573171615600586, 8.361051559448242, 13.788335800170898, 10.328788757324219], 12995: [10.008891105651855, 5.7023468017578125], 5051: [9.601136207580566], 11063: [7.630705833435059], 5261: [7.4960808753967285], 15184: [7.159242630004883, 18.580429077148438, 8.097074508666992], 5342: [6.515382289886475, 9.317991256713867], 11174: [19.608783721923828], 911: [16.78046226501465], 1409: [10.811419486999512], 1134: [10.570173263549805], 11505: [10.384431838989258], 11165: [8.330888748168945], 15063: [7.956270694732666], 5264: [21.729684829711914, 22.13262939453125, 17.989295959472656, 17.561262130737305], 476: [17.005456924438477, 16.191181182861328], 12168: [14.771980285644531, 9.768433570861816, 10.76189136505127], 14318: [10.751014709472656, 8.285650253295898], 14028: [8.511176109313965, 9.473976135253906, 6.865013122558594], 12455: [7.600131034851074, 9.226279258728027], 11652: [21.650659561157227, 14.224082946777344], 7147: [13.388426780700684], 11414: [10.082221984863281], 9358: [9.826055526733398], 11752: [9.429441452026367, 12.592951774597168], 3437: [8.818697929382324], 1709: [8.473386764526367, 12.564458847045898, 8.345649719238281], 8308: [7.197636604309082], 1719: [6.923713684082031], 13512: [29.341312408447266], 7680: [15.37956714630127, 16.033893585205078], 11234: [14.180534362792969], 2466: [10.200555801391602], 10770: [9.810698509216309], 5149: [9.08472728729248, 11.651917457580566], 10004: [8.969457626342773], 15646: [9.036194801330566], 6577: [21.56995964050293], 7369: [15.67188835144043], 12507: [15.395538330078125], 15086: [15.23991870880127], 7515: [17.202726364135742], 12764: [16.47142791748047], 5033: [14.958761215209961], 11082: [10.783834457397461], 3405: [10.240108489990234], 12371: [8.173274993896484], 13968: [33.39778518676758], 9768: [7.012979507446289], 15173: [20.68423080444336], 4474: [11.086263656616211], 3284: [10.281087875366211], 3719: [10.035972595214844], 9048: [9.60767936706543, 16.832149505615234], 1202: [8.405071258544922], 9115: [7.614710330963135], 4593: [13.206755638122559], 16212: [12.275304794311523], 4609: [6.763081073760986], 14385: [6.743544578552246], 4737: [5.455965042114258], 4639: [5.404547214508057], 1976: [5.349384307861328], 609: [26.93107795715332], 15160: [16.65496063232422], 11088: [12.32497787475586], 15803: [10.472724914550781], 13071: [7.997514724731445], 3067: [7.016715049743652], 5041: [9.047438621520996], 13889: [8.019342422485352], 11422: [7.30921745300293], 14860: [5.0690107345581055], 14648: [5.033542633056641], 9253: [6.975615501403809, 10.044901847839355], 9792: [5.341488838195801], 13699: [7.649077415466309, 6.357934951782227], 7354: [7.423045635223389], 2605: [5.528987884521484], 10297: [5.130509376525879], 13266: [7.644717693328857], 16130: [7.3850016593933105], 5862: [6.438265800476074], 10219: [6.307385444641113], 12349: [5.9273786544799805, 6.762894630432129], 11543: [8.605957984924316, 6.393813133239746], 7405: [8.378878593444824], 8516: [20.980695724487305], 15444: [20.553966522216797], 3671: [18.769887924194336], 11657: [12.29224967956543], 11494: [10.248099327087402], 1889: [10.244827270507812], 320: [6.309274673461914], 13368: [6.587907791137695], 154: [6.322232246398926], 3578: [12.670225143432617], 5130: [7.106779098510742], 3602: [5.700640678405762], 9244: [20.28443717956543], 9143: [11.393896102905273], 16332: [6.886565685272217], 7178: [6.042776107788086], 2599: [5.974696636199951], 8684: [8.165029525756836], 11989: [8.052072525024414], 2804: [6.119560718536377], 2561: [5.963934898376465], 9619: [6.0411272048950195], 9559: [20.749343872070312], 4168: [19.552663803100586], 9227: [12.031373977661133], 13775: [9.777322769165039], 9088: [8.281668663024902], 6793: [8.170025825500488], 16127: [7.410332202911377], 12947: [7.3633503913879395], 15743: [13.567876815795898], 4631: [9.135679244995117], 1979: [6.229153156280518], 6143: [6.010380268096924], 2725: [8.132755279541016], 14002: [6.501742362976074], 1045: [9.419561386108398], 13933: [7.895888328552246], 893: [7.691374778747559], 2001: [7.6114277839660645], 13639: [7.502982139587402], 8961: [8.845383644104004], 7040: [8.092082977294922], 7941: [8.445110321044922], 5948: [8.234968185424805], 8176: [6.536914348602295], 7179: [22.127151489257812], 483: [21.650264739990234], 14056: [15.432649612426758], 3435: [13.089166641235352], 15168: [11.144266128540039], 10513: [9.403603553771973], 2628: [22.835649490356445], 8201: [22.245464324951172], 10451: [17.319414138793945], 13348: [14.374091148376465], 5428: [12.158997535705566], 1292: [9.939239501953125], 13531: [9.415156364440918], 14245: [9.258074760437012], 1852: [9.17228889465332], 6311: [12.789658546447754], 6858: [12.317315101623535, 7.9506940841674805], 14651: [11.545376777648926], 7549: [10.741230964660645], 7788: [8.61984920501709], 6978: [8.235995292663574], 14312: [8.187301635742188], 6990: [7.3854756355285645], 8023: [6.731232643127441], 13177: [8.828405380249023], 12113: [8.115386962890625], 10549: [9.804838180541992], 16191: [9.43216323852539], 11823: [8.555636405944824], 8665: [9.946271896362305], 1075: [7.113838195800781], 12320: [6.9286627769470215], 10102: [11.719564437866211], 10973: [8.444698333740234], 10718: [8.51451301574707], 6167: [12.644335746765137], 1144: [9.640304565429688], 9718: [8.714523315429688], 1271: [15.274971961975098], 12867: [9.323521614074707], 8225: [8.41750717163086], 10895: [6.463388919830322], 11724: [6.269600868225098], 4378: [5.758092403411865], 3618: [12.005608558654785], 12293: [9.357418060302734, 5.593735694885254], 9387: [7.979825496673584], 4851: [6.507242202758789], 1634: [6.940520286560059], 7937: [15.325420379638672], 5743: [9.417481422424316], 12333: [8.270907402038574], 2530: [6.881093978881836], 7031: [9.993199348449707], 11889: [7.34765100479126]}\n"
     ]
    }
   ],
   "source": [
    "def filter_neurons(top_neurons_neg, top_neurons_pos, threshold=5.0):\n",
    "    \"\"\"\n",
    "    Filters out neurons that are highly activated in both the negative and positive sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    filtered_neurons_neg = {}\n",
    "    filtered_neurons_pos = {}\n",
    "\n",
    "    for neuron, activations in top_neurons_neg.items():\n",
    "        if neuron in top_neurons_pos and any(val >= threshold for val in activations) and any(val >= threshold for val in top_neurons_pos[neuron]):\n",
    "            continue \n",
    "        else:\n",
    "            filtered_neurons_neg[neuron] = activations\n",
    "\n",
    "    for neuron, activations in top_neurons_pos.items():\n",
    "        if neuron not in top_neurons_neg or not any(val >= threshold for val in top_neurons_neg[neuron]):\n",
    "            filtered_neurons_pos[neuron] = activations\n",
    "\n",
    "    return filtered_neurons_neg, filtered_neurons_pos\n",
    "\n",
    "filtered_neg, filtered_pos = filter_neurons(top_neurons_neg, top_neurons_pos, 0)\n",
    "print(f\"Len: {len(filtered_neg)}. Filtered negative neurons: {filtered_neg}\")\n",
    "print(f\"Len: {len(filtered_pos)}. Filtered positive neurons: {filtered_pos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{15292: 6, 3379: 8, 13546: 15, 2404: 2, 8866: 2, 91: 8, 11992: 5, 3965: 7, 5734: 1, 10623: 8, 11116: 1, 2471: 1, 14622: 2, 12446: 12, 1955: 1, 4059: 1, 4956: 3, 4603: 4, 3738: 1, 5525: 9, 13504: 10, 13676: 2, 9866: 7, 976: 1, 10872: 1, 5363: 6, 12962: 2, 13321: 1, 16333: 1, 15759: 3, 13304: 2, 26: 1, 12411: 1, 13399: 5, 607: 3, 4565: 1, 2780: 4, 10248: 1, 4460: 2, 10319: 3, 1495: 34, 5458: 13, 3870: 2, 12505: 1, 15701: 2, 6286: 1, 3394: 1, 6451: 3, 11203: 1, 10468: 2, 1323: 2, 13087: 1, 11148: 2, 4664: 4, 2974: 4, 7241: 1, 4255: 1, 13474: 5, 5370: 2, 13307: 1, 1530: 8, 13388: 1, 8420: 11, 9223: 3, 189: 1, 16203: 4, 820: 1, 5351: 2, 10606: 2, 10805: 2, 6638: 1, 13862: 1, 9198: 2, 3069: 2, 6447: 1, 10443: 1, 15794: 2, 12825: 6, 12570: 1, 9780: 1, 10547: 1, 5873: 1, 7047: 5, 9020: 3, 11274: 1, 1389: 1, 6231: 1, 11620: 1, 3568: 1, 1648: 1, 5785: 3, 7110: 1, 550: 1, 6163: 1, 3556: 3, 4322: 1, 6292: 1, 6770: 11, 10930: 2, 8800: 4, 13218: 5, 13350: 1, 6948: 1, 9000: 1, 12772: 1, 12042: 2, 14430: 1, 6561: 1, 10400: 7, 7091: 2, 14220: 1, 859: 1, 10548: 4, 10097: 1, 3945: 1, 14449: 10, 7069: 5, 15460: 1, 15678: 2, 11: 4, 13887: 3, 14118: 10, 14826: 3, 7222: 2, 12124: 2, 6855: 2, 2811: 2, 3329: 1, 5125: 5, 16174: 1, 9125: 3, 6403: 2, 4662: 1, 3347: 1, 2085: 1, 5653: 1, 1373: 1, 7566: 7, 10908: 3, 12065: 3, 4406: 1, 2876: 3, 12442: 2, 7517: 2, 11264: 1, 10894: 2, 5751: 1, 13343: 2, 10726: 1, 14200: 2, 6015: 1, 4886: 1, 5669: 2, 3297: 1, 6998: 2, 2416: 2, 3849: 1, 2550: 1, 2086: 1, 9905: 1, 15949: 1, 15802: 1, 9157: 1, 15598: 1, 16009: 3, 15568: 1, 15029: 1, 542: 1, 12742: 1, 6632: 4, 12961: 1, 10078: 1, 13430: 1, 2401: 3, 3923: 2, 18: 2, 2313: 2, 13379: 3, 14326: 5, 11630: 6, 15268: 3, 3705: 2, 11525: 2, 2192: 1, 15205: 2, 1053: 1, 11566: 1, 6836: 1, 5982: 2, 15351: 1, 644: 5, 10348: 1, 13746: 2, 620: 1, 5486: 1, 5835: 2, 12957: 2, 9459: 1, 5644: 2, 13339: 2, 3326: 2, 3039: 1, 5409: 4, 2648: 3, 10236: 3, 12755: 1, 12044: 1, 7203: 1, 6767: 3, 10534: 2, 3233: 3, 2908: 1, 9859: 1, 7862: 1, 13407: 2, 3554: 1, 7726: 1, 4078: 1, 7030: 2, 2691: 1, 11820: 4, 9454: 1, 954: 1, 9942: 3, 9089: 1, 10994: 1, 9420: 1, 9787: 3, 14880: 2, 3779: 3, 8889: 3, 5786: 1, 1849: 1, 1402: 2, 14846: 2, 8751: 2, 3066: 1, 9874: 1, 10429: 1, 8317: 1, 7787: 1, 11353: 2, 15626: 1, 2268: 2, 2861: 1, 418: 2, 8973: 1, 10228: 2, 3895: 1, 5173: 1, 16271: 1, 8781: 1, 4356: 1, 12614: 2, 14988: 3, 7799: 1, 3130: 3, 7266: 1, 2707: 1, 8967: 1, 10603: 1, 10650: 1, 4608: 1, 11098: 1, 2856: 1, 10840: 1, 6172: 2, 11440: 2, 9699: 2, 11833: 1, 13155: 2, 1941: 2, 8701: 1, 9292: 1, 2719: 1, 12922: 1, 15863: 1, 16204: 1, 8110: 1, 5840: 1, 14730: 1, 3726: 1, 4449: 1, 2985: 1, 5454: 1, 5621: 3, 13066: 2, 3832: 1, 9762: 1, 9944: 1, 8424: 1, 1334: 1, 6394: 1, 14069: 2, 11673: 1, 2781: 1, 15753: 1, 12819: 1, 10749: 1, 14212: 1, 12284: 1, 8765: 2, 14655: 2, 4961: 1, 1678: 1, 10380: 1, 7681: 1, 12137: 1, 9240: 1, 9796: 1, 6137: 1, 14892: 1, 4107: 1, 6809: 1, 15362: 1, 9930: 1, 13476: 1, 2318: 2, 8878: 1, 13795: 1, 4081: 1, 7538: 1, 8676: 3, 1514: 1, 15092: 2, 7693: 1, 12428: 1, 9882: 1, 7836: 1, 8382: 2, 6893: 2, 6659: 2, 7948: 1, 5085: 1, 7377: 1, 1235: 1, 9498: 2, 2400: 1, 8531: 1, 12484: 1, 7271: 1, 4872: 1, 14253: 2, 1266: 1, 8166: 1, 7617: 2, 5662: 1, 9350: 1, 30: 2, 1689: 1, 4782: 1, 13653: 2, 5856: 1, 7144: 1, 3287: 1, 6631: 1, 5070: 1, 4583: 1, 16044: 1, 13249: 2, 1264: 1, 10398: 1, 2518: 1, 4877: 1, 9916: 1, 6974: 1, 798: 1, 8515: 1, 2050: 1, 6564: 1, 152: 1, 3918: 1, 5833: 2, 11766: 1, 9609: 1, 12355: 2, 13556: 1, 2752: 1, 14339: 1, 2354: 1, 4327: 2, 4938: 1, 11052: 1, 2924: 1, 3866: 1, 5920: 1, 14304: 1, 6340: 1, 5578: 1, 9275: 1, 1193: 1, 15648: 1, 13601: 1, 1823: 1, 2870: 1, 1756: 1, 11144: 1, 11536: 1, 15111: 1, 1813: 1, 10806: 1, 14208: 1, 2732: 1, 7613: 1, 11600: 1, 14394: 1, 2990: 1, 14928: 1, 6837: 1, 5322: 1, 6936: 1, 7988: 1, 3969: 1, 9564: 1, 12684: 1, 7961: 1, 15935: 1, 3715: 1, 10735: 1, 5037: 1, 7944: 1, 8125: 1, 3943: 1, 4235: 1, 11960: 1, 14632: 2, 1640: 1, 9259: 1, 12898: 1, 9386: 1, 3887: 2, 10676: 1, 14514: 1, 13255: 1, 10430: 1, 407: 1, 13582: 1, 5381: 1, 14924: 1, 2015: 1, 14506: 1, 14433: 1, 7017: 1, 8205: 1, 2515: 1, 1598: 1, 16200: 2, 2754: 1, 13741: 1, 2756: 1, 3581: 1, 7390: 1, 16100: 2, 14666: 1, 15098: 2, 5912: 3, 3507: 1, 13980: 2, 5588: 1, 228: 1, 11094: 1, 10203: 1, 15792: 2, 4227: 1, 5111: 1, 8499: 1, 12051: 1, 8460: 1, 2136: 2, 3560: 1, 11803: 1, 1636: 1, 9666: 1, 955: 1, 12308: 1, 11708: 1, 1092: 1, 11837: 1, 3510: 1, 5449: 1, 12665: 1, 9073: 1, 14063: 1, 2595: 1, 10581: 1, 12086: 1, 14941: 1, 10807: 1, 2218: 1, 5355: 1, 12823: 2, 10082: 1, 14417: 1, 15310: 2, 8027: 1, 6532: 1, 7765: 1, 12741: 1, 4191: 1, 7880: 1, 1781: 1, 13115: 1, 513: 1, 4714: 1, 11695: 1, 6556: 1, 6076: 1, 5829: 1, 6926: 1, 2442: 1, 9151: 1, 16293: 1, 11242: 1}\n",
      "{3288: 35, 7081: 12, 1909: 1, 2652: 14, 14062: 2, 14810: 4, 10247: 12, 16067: 1, 12277: 1, 16116: 15, 5807: 6, 5325: 12, 16225: 1, 13270: 52, 11141: 15, 8591: 1, 6493: 23, 15601: 3, 43: 20, 4543: 2, 8504: 2, 15185: 2, 8387: 1, 8810: 1, 12616: 5, 4092: 2, 2308: 1, 11256: 1, 1476: 1, 11998: 1, 8014: 3, 8388: 1, 9095: 1, 9610: 2, 13766: 2, 636: 4, 12814: 1, 9706: 1, 5997: 2, 6025: 1, 14930: 1, 11018: 1, 14285: 1, 6160: 1, 12620: 1, 15040: 1, 10198: 1, 12917: 1, 14609: 1, 1428: 1, 16064: 2, 2366: 7, 1622: 1, 512: 1, 15084: 2, 8544: 3, 11157: 11, 13201: 1, 9291: 1, 10556: 1, 7328: 1, 7639: 1, 16340: 32, 6777: 3, 4642: 1, 5608: 3, 13727: 1, 7915: 3, 4070: 1, 7019: 1, 1081: 2, 42: 2, 7894: 1, 6085: 2, 3053: 1, 11247: 16, 13004: 3, 13991: 22, 3932: 1, 6870: 12, 6139: 2, 1739: 3, 8414: 1, 11084: 1, 2583: 1, 7140: 2, 2542: 1, 4683: 3, 5382: 1, 7028: 7, 12506: 3, 12062: 1, 2043: 1, 3176: 1, 10618: 1, 11556: 1, 7129: 16, 14265: 1, 869: 5, 8346: 1, 14590: 1, 4586: 1, 4161: 1, 11034: 6, 7682: 1, 7118: 2, 2059: 6, 5683: 5, 6885: 1, 16075: 8, 12740: 4, 7012: 3, 6130: 9, 901: 2, 9365: 1, 6141: 1, 11865: 6, 14421: 3, 14798: 1, 3177: 7, 8444: 1, 10155: 1, 13881: 4, 4263: 1, 12156: 1, 9917: 1, 14680: 1, 8686: 2, 10988: 2, 9113: 2, 1385: 5, 4171: 1, 3596: 1, 7607: 1, 15504: 2, 9592: 1, 2096: 3, 15232: 5, 12880: 2, 11040: 2, 15835: 1, 3975: 3, 8120: 2, 3649: 2, 5386: 1, 4852: 2, 15399: 1, 14665: 4, 6386: 1, 3867: 1, 8310: 1, 10405: 2, 1265: 1, 10391: 1, 13768: 1, 9463: 1, 1348: 3, 8192: 1, 12631: 1, 13000: 1, 3263: 1, 8861: 1, 4522: 1, 14674: 2, 1059: 1, 10087: 2, 4809: 2, 13738: 1, 7702: 2, 4791: 4, 1360: 2, 7841: 1, 6152: 1, 12629: 5, 1919: 1, 7710: 2, 11727: 1, 6496: 1, 10816: 2, 10969: 3, 2109: 2, 15237: 1, 12936: 1, 9321: 1, 1425: 2, 8234: 2, 7210: 1, 7479: 1, 6627: 1, 1978: 1, 2288: 1, 1261: 1, 15502: 2, 14542: 1, 13192: 1, 15039: 1, 14903: 1, 12619: 2, 14792: 1, 6542: 1, 11488: 1, 6209: 1, 7977: 1, 8111: 3, 12032: 4, 2672: 1, 11551: 1, 3653: 1, 8093: 3, 10931: 8, 8991: 3, 11336: 2, 8457: 2, 3078: 1, 13898: 1, 9547: 3, 2035: 1, 11444: 2, 9148: 6, 11929: 1, 15282: 2, 12794: 1, 2293: 1, 6304: 2, 6780: 1, 14038: 1, 423: 1, 3217: 2, 1892: 2, 9899: 6, 10243: 3, 7544: 1, 13222: 1, 13751: 1, 11463: 1, 15692: 1, 8885: 1, 15484: 1, 12600: 1, 9898: 1, 193: 3, 7650: 1, 658: 1, 13743: 2, 5427: 2, 13663: 1, 10503: 6, 12161: 1, 2211: 2, 15476: 1, 10265: 1, 9824: 10, 3094: 1, 8464: 1, 11328: 1, 11869: 1, 1236: 2, 10517: 4, 16070: 1, 14856: 1, 4537: 1, 13401: 6, 2766: 1, 716: 8, 2306: 1, 12792: 1, 13084: 1, 13475: 1, 3696: 5, 1255: 1, 4983: 1, 2965: 4, 9766: 1, 15336: 2, 11891: 3, 15770: 2, 4111: 1, 13020: 1, 2116: 1, 12513: 1, 14617: 2, 5675: 1, 6937: 1, 12049: 1, 9023: 1, 7380: 1, 3459: 1, 4704: 2, 3695: 1, 9299: 1, 6640: 1, 3736: 2, 14372: 2, 15638: 2, 381: 2, 10115: 3, 9744: 2, 0: 2, 1: 2, 2: 2, 3: 2, 4: 2, 6: 1, 2753: 1, 1969: 1, 11057: 1, 10508: 1, 2969: 1, 1582: 1, 12730: 1, 5403: 1, 12458: 1, 1234: 1, 15136: 3, 7145: 1, 12316: 1, 15469: 1, 11953: 3, 10105: 1, 11407: 1, 6850: 1, 510: 1, 11000: 1, 4291: 1, 5074: 1, 4024: 1, 10785: 1, 14216: 1, 13747: 1, 5854: 1, 2588: 2, 12261: 1, 12653: 1, 7814: 1, 10431: 1, 15620: 1, 16132: 2, 1945: 1, 2330: 1, 5099: 1, 15507: 1, 11426: 1, 281: 1, 12909: 1, 6389: 1, 2552: 1, 9567: 1, 6125: 1, 15312: 1, 9936: 1, 10821: 1, 13093: 2, 12103: 1, 15044: 2, 4027: 2, 9127: 1, 6901: 2, 14271: 1, 10411: 1, 15862: 1, 11716: 2, 10972: 1, 4611: 1, 47: 1, 921: 1, 10798: 1, 9371: 1, 32: 2, 16023: 1, 15922: 1, 14747: 1, 9453: 1, 6903: 1, 12377: 1, 15921: 1, 13996: 1, 4643: 1, 14114: 1, 6875: 2, 15506: 1, 2721: 1, 13809: 1, 13975: 1, 9956: 1, 2525: 1, 3566: 1, 12787: 1, 1188: 7, 15663: 1, 5155: 1, 6490: 1, 7189: 1, 15510: 1, 15938: 1, 6287: 1, 14661: 2, 224: 1, 6444: 1, 6539: 2, 1834: 1, 4187: 3, 7348: 1, 5907: 1, 5230: 1, 11051: 1, 6887: 2, 11565: 1, 13647: 1, 11387: 1, 16129: 1, 11777: 1, 3495: 1, 3952: 1, 9835: 1, 1019: 2, 700: 1, 16261: 3, 5981: 1, 7330: 1, 9760: 1, 7519: 1, 2032: 1, 13960: 1, 4358: 1, 5456: 1, 4119: 1, 13054: 5, 2279: 1, 1535: 1, 10689: 1, 15449: 1, 12226: 1, 156: 2, 11348: 1, 10923: 1, 3822: 1, 601: 1, 14769: 1, 1851: 1, 2004: 1, 14448: 1, 655: 1, 13857: 1, 8946: 2, 15674: 1, 15266: 1, 2562: 1, 8898: 1, 13319: 1, 4202: 1, 13978: 1, 12983: 1, 8381: 1, 10100: 1, 9969: 3, 7943: 1, 8886: 1, 2349: 1, 2961: 1, 7397: 1, 7088: 1, 15781: 1, 1942: 1, 11287: 1, 11149: 1, 1620: 1, 13226: 2, 2007: 1, 11138: 1, 7869: 1, 13694: 1, 4849: 1, 634: 1, 881: 1, 5740: 1, 5881: 1, 6714: 2, 176: 3, 6795: 5, 12995: 2, 5051: 1, 11063: 1, 5261: 1, 15184: 3, 5342: 2, 11174: 1, 911: 1, 1409: 1, 1134: 1, 11505: 1, 11165: 1, 15063: 1, 5264: 4, 476: 2, 12168: 3, 14318: 2, 14028: 3, 12455: 2, 11652: 2, 7147: 1, 11414: 1, 9358: 1, 11752: 2, 3437: 1, 1709: 3, 8308: 1, 1719: 1, 13512: 1, 7680: 2, 11234: 1, 2466: 1, 10770: 1, 5149: 2, 10004: 1, 15646: 1, 6577: 1, 7369: 1, 12507: 1, 15086: 1, 7515: 1, 12764: 1, 5033: 1, 11082: 1, 3405: 1, 12371: 1, 13968: 1, 9768: 1, 15173: 1, 4474: 1, 3284: 1, 3719: 1, 9048: 2, 1202: 1, 9115: 1, 4593: 1, 16212: 1, 4609: 1, 14385: 1, 4737: 1, 4639: 1, 1976: 1, 609: 1, 15160: 1, 11088: 1, 15803: 1, 13071: 1, 3067: 1, 5041: 1, 13889: 1, 11422: 1, 14860: 1, 14648: 1, 9253: 2, 9792: 1, 13699: 2, 7354: 1, 2605: 1, 10297: 1, 13266: 1, 16130: 1, 5862: 1, 10219: 1, 12349: 2, 11543: 2, 7405: 1, 8516: 1, 15444: 1, 3671: 1, 11657: 1, 11494: 1, 1889: 1, 320: 1, 13368: 1, 154: 1, 3578: 1, 5130: 1, 3602: 1, 9244: 1, 9143: 1, 16332: 1, 7178: 1, 2599: 1, 8684: 1, 11989: 1, 2804: 1, 2561: 1, 9619: 1, 9559: 1, 4168: 1, 9227: 1, 13775: 1, 9088: 1, 6793: 1, 16127: 1, 12947: 1, 15743: 1, 4631: 1, 1979: 1, 6143: 1, 2725: 1, 14002: 1, 1045: 1, 13933: 1, 893: 1, 2001: 1, 13639: 1, 8961: 1, 7040: 1, 7941: 1, 5948: 1, 8176: 1, 7179: 1, 483: 1, 14056: 1, 3435: 1, 15168: 1, 10513: 1, 2628: 1, 8201: 1, 10451: 1, 13348: 1, 5428: 1, 1292: 1, 13531: 1, 14245: 1, 1852: 1, 6311: 1, 6858: 2, 14651: 1, 7549: 1, 7788: 1, 6978: 1, 14312: 1, 6990: 1, 8023: 1, 13177: 1, 12113: 1, 10549: 1, 16191: 1, 11823: 1, 8665: 1, 1075: 1, 12320: 1, 10102: 1, 10973: 1, 10718: 1, 6167: 1, 1144: 1, 9718: 1, 1271: 1, 12867: 1, 8225: 1, 10895: 1, 11724: 1, 4378: 1, 3618: 1, 12293: 2, 9387: 1, 4851: 1, 1634: 1, 7937: 1, 5743: 1, 12333: 1, 2530: 1, 7031: 1, 11889: 1}\n",
      "[(1495, 34), (13546, 15), (5458, 13), (12446, 12), (8420, 11), (6770, 11), (13504, 10), (14449, 10), (14118, 10), (5525, 9), (3379, 8), (91, 8), (10623, 8), (1530, 8), (3965, 7), (9866, 7), (10400, 7), (7566, 7), (15292, 6), (5363, 6), (12825, 6), (11630, 6), (11992, 5), (13399, 5), (13474, 5), (7047, 5), (13218, 5), (7069, 5), (5125, 5), (14326, 5), (644, 5), (4603, 4), (2780, 4), (4664, 4), (2974, 4), (16203, 4), (8800, 4), (10548, 4), (11, 4), (6632, 4), (5409, 4), (11820, 4), (4956, 3), (15759, 3), (607, 3), (10319, 3), (6451, 3), (9223, 3), (9020, 3), (5785, 3), (3556, 3), (13887, 3), (14826, 3), (9125, 3), (10908, 3), (12065, 3), (2876, 3), (16009, 3), (2401, 3), (13379, 3), (15268, 3), (2648, 3), (10236, 3), (6767, 3), (3233, 3), (9942, 3), (9787, 3), (3779, 3), (8889, 3), (14988, 3), (3130, 3), (5621, 3), (8676, 3), (5912, 3), (2404, 2), (8866, 2), (14622, 2), (13676, 2), (12962, 2), (13304, 2), (4460, 2), (3870, 2), (15701, 2), (10468, 2), (1323, 2), (11148, 2), (5370, 2), (5351, 2), (10606, 2), (10805, 2), (9198, 2), (3069, 2), (15794, 2), (10930, 2), (12042, 2), (7091, 2), (15678, 2), (7222, 2), (12124, 2), (6855, 2), (2811, 2), (6403, 2), (12442, 2), (7517, 2), (10894, 2), (13343, 2), (14200, 2), (5669, 2), (6998, 2), (2416, 2), (3923, 2), (18, 2), (2313, 2), (3705, 2), (11525, 2), (15205, 2), (5982, 2), (13746, 2), (5835, 2), (12957, 2), (5644, 2), (13339, 2), (3326, 2), (10534, 2), (13407, 2), (7030, 2), (14880, 2), (1402, 2), (14846, 2), (8751, 2), (11353, 2), (2268, 2), (418, 2), (10228, 2), (12614, 2), (6172, 2), (11440, 2), (9699, 2), (13155, 2), (1941, 2), (13066, 2), (14069, 2), (8765, 2), (14655, 2), (2318, 2), (15092, 2), (8382, 2), (6893, 2), (6659, 2), (9498, 2), (14253, 2), (7617, 2), (30, 2), (13653, 2), (13249, 2), (5833, 2), (12355, 2), (4327, 2), (14632, 2), (3887, 2), (16200, 2), (16100, 2), (15098, 2), (13980, 2), (15792, 2), (2136, 2), (12823, 2), (15310, 2), (5734, 1), (11116, 1), (2471, 1), (1955, 1), (4059, 1), (3738, 1), (976, 1), (10872, 1), (13321, 1), (16333, 1), (26, 1), (12411, 1), (4565, 1), (10248, 1), (12505, 1), (6286, 1), (3394, 1), (11203, 1), (13087, 1), (7241, 1), (4255, 1), (13307, 1), (13388, 1), (189, 1), (820, 1), (6638, 1), (13862, 1), (6447, 1), (10443, 1), (12570, 1), (9780, 1), (10547, 1)]\n",
      "[(13270, 52), (3288, 35), (16340, 32), (6493, 23), (13991, 22), (43, 20), (11247, 16), (7129, 16), (16116, 15), (11141, 15), (2652, 14), (7081, 12), (10247, 12), (5325, 12), (6870, 12), (11157, 11), (9824, 10), (6130, 9), (16075, 8), (10931, 8), (716, 8), (2366, 7), (7028, 7), (3177, 7), (1188, 7), (5807, 6), (11034, 6), (2059, 6), (11865, 6), (9148, 6), (9899, 6), (10503, 6), (13401, 6), (12616, 5), (869, 5), (5683, 5), (1385, 5), (15232, 5), (12629, 5), (3696, 5), (13054, 5), (6795, 5), (14810, 4), (636, 4), (12740, 4), (13881, 4), (14665, 4), (4791, 4), (12032, 4), (10517, 4), (2965, 4), (5264, 4), (15601, 3), (8014, 3), (8544, 3), (6777, 3), (5608, 3), (7915, 3), (13004, 3), (1739, 3), (4683, 3), (12506, 3), (7012, 3), (14421, 3), (2096, 3), (3975, 3), (1348, 3), (10969, 3), (8111, 3), (8093, 3), (8991, 3), (9547, 3), (10243, 3), (193, 3), (11891, 3), (10115, 3), (15136, 3), (11953, 3), (4187, 3), (16261, 3), (9969, 3), (176, 3), (15184, 3), (12168, 3), (14028, 3), (1709, 3), (14062, 2), (4543, 2), (8504, 2), (15185, 2), (4092, 2), (9610, 2), (13766, 2), (5997, 2), (16064, 2), (15084, 2), (1081, 2), (42, 2), (6085, 2), (6139, 2), (7140, 2), (7118, 2), (901, 2), (8686, 2), (10988, 2), (9113, 2), (15504, 2), (12880, 2), (11040, 2), (8120, 2), (3649, 2), (4852, 2), (10405, 2), (14674, 2), (10087, 2), (4809, 2), (7702, 2), (1360, 2), (7710, 2), (10816, 2), (2109, 2), (1425, 2), (8234, 2), (15502, 2), (12619, 2), (11336, 2), (8457, 2), (11444, 2), (15282, 2), (6304, 2), (3217, 2), (1892, 2), (13743, 2), (5427, 2), (2211, 2), (1236, 2), (15336, 2), (15770, 2), (14617, 2), (4704, 2), (3736, 2), (14372, 2), (15638, 2), (381, 2), (9744, 2), (0, 2), (1, 2), (2, 2), (3, 2), (4, 2), (2588, 2), (16132, 2), (13093, 2), (15044, 2), (4027, 2), (6901, 2), (11716, 2), (32, 2), (6875, 2), (14661, 2), (6539, 2), (6887, 2), (1019, 2), (156, 2), (8946, 2), (13226, 2), (6714, 2), (12995, 2), (5342, 2), (476, 2), (14318, 2), (12455, 2), (11652, 2), (11752, 2), (7680, 2), (5149, 2), (9048, 2), (9253, 2), (13699, 2), (12349, 2), (11543, 2), (6858, 2), (12293, 2), (1909, 1), (16067, 1), (12277, 1), (16225, 1), (8591, 1), (8387, 1), (8810, 1), (2308, 1), (11256, 1), (1476, 1), (11998, 1), (8388, 1), (9095, 1), (12814, 1), (9706, 1), (6025, 1), (14930, 1)]\n"
     ]
    }
   ],
   "source": [
    "# average activations over each top case, sends to\n",
    "# top_neurons_neg/pos = {idx: avg_act, idx2:avg_act2, ...}\n",
    "top_neurons_neg_mean = {}\n",
    "for entry in filtered_neg:\n",
    "    top_neurons_neg_mean[entry] = len(filtered_neg[entry])\n",
    "\n",
    "top_neurons_pos_mean = {}\n",
    "for entry in filtered_pos:\n",
    "    top_neurons_pos_mean[entry] = len(filtered_pos[entry])\n",
    "\n",
    "print(top_neurons_neg_mean)\n",
    "print(top_neurons_pos_mean)\n",
    "\n",
    "# sort by avg activation\n",
    "top_neurons_neg_mean = {k: v for k, v in sorted(top_neurons_neg_mean.items(), key=lambda item: item[1], reverse=True)}\n",
    "top_neurons_pos_mean = {k: v for k, v in sorted(top_neurons_pos_mean.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "\n",
    "# print first few\n",
    "print(list(top_neurons_neg_mean.items())[:200])\n",
    "print(list(top_neurons_pos_mean.items())[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9250\n"
     ]
    }
   ],
   "source": [
    "# train classifier on sae activations\n",
    "activations_list = []\n",
    "labels_list = []\n",
    "\n",
    "# 0 = negative, 1 = positive\n",
    "for example_txt in negative_set:\n",
    "    _, cache = model.run_with_cache_with_saes(example_txt, saes=[sae])\n",
    "    activations = cache[f'blocks.{num_layer}.hook_resid_post.hook_sae_acts_post'][0, -1, :].cpu().numpy()\n",
    "    #print(activations.shape)\n",
    "\n",
    "    del cache\n",
    "\n",
    "    activations_list.append(activations)\n",
    "    labels_list.append(0)\n",
    "\n",
    "for example_txt in positive_set:\n",
    "    _, cache = model.run_with_cache_with_saes(example_txt, saes=[sae])\n",
    "    activations = cache[f'blocks.{num_layer}.hook_resid_post.hook_sae_acts_post'][0, -1, :].cpu().numpy()\n",
    "\n",
    "    del cache\n",
    "\n",
    "    activations_list.append(activations)\n",
    "    labels_list.append(1)   \n",
    "\n",
    "# data\n",
    "X = np.array(activations_list)\n",
    "y = np.array(labels_list)\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scale activation features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, solver='lbfgs') \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "with open('run_log.txt', 'a') as file:\n",
    "    file.write(f'SAE activations linear classifier accuracy on pos={pos_dataset} neg={neg_dataset} layer={num_layer} dataset_size={NEG_SET_SIZE}: {accuracy:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9875\n",
      "Test Accuracy: 0.9875\n"
     ]
    }
   ],
   "source": [
    "# train classifier on base activations\n",
    "activations_list = []\n",
    "labels_list = []\n",
    "\n",
    "# 0 = negative, 1 = positive\n",
    "for example_txt in negative_set:\n",
    "    _, cache = model.run_with_cache(example_txt)\n",
    "    res_stream = cache.decompose_resid(layer=15, return_labels=False, mode='attn', incl_embeds=False, pos_slice=slice(-2, -1))\n",
    "    seven_out = res_stream[-1, 0, -1, :].cpu().numpy() # layer batch pos d_model\n",
    "    #print(seven_out.shape)\n",
    "\n",
    "    del cache\n",
    "\n",
    "    activations_list.append(seven_out)\n",
    "    labels_list.append(0)\n",
    "\n",
    "#print(\"done w bad\")\n",
    "\n",
    "for example_txt in positive_set:\n",
    "    _, cache = model.run_with_cache(example_txt)\n",
    "    res_stream = cache.decompose_resid(layer=15, return_labels=False, mode='attn', incl_embeds=False, pos_slice=slice(-2, -1))\n",
    "    seven_out = res_stream[-1, 0, -1, :].cpu().numpy()  # layer batch pos d_model\n",
    "\n",
    "    del cache\n",
    "\n",
    "    activations_list.append(seven_out)\n",
    "    labels_list.append(1)\n",
    "\n",
    "# data and split\n",
    "X = np.array(activations_list)\n",
    "y = np.array(labels_list)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scale features \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# train classifier\n",
    "clf = LogisticRegression(max_iter=1000, solver='lbfgs') \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "with open('run_log.txt', 'a') as file:\n",
    "    file.write(f'Base activations linear classifier accuracy on pos={pos_dataset} neg={neg_dataset} layer={num_layer} dataset_size={NEG_SET_SIZE}: {accuracy:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1495, 5154, 14326, 12446, 6770, 2405, 840, 13399, 15794, 8420, 3069, 14622, 13430, 4603, 8382, 9882, 13407, 11992, 11630, 4956, 9189, 13887, 14118, 3379, 8711, 16271, 3965, 14449, 14660, 15268, 13474, 6659, 4963, 15292, 1314, 10228, 2471, 16200, 10623, 6571, 13863, 10430, 7273, 7681, 5840, 9125, 5409, 11820, 4228, 9447]\n",
      " RATS !  97.0\n",
      " GENIUSES !  14.5\n"
     ]
    }
   ],
   "source": [
    "# reject sample if activation at any of top N neurons is above threshold T\n",
    "THRESHOLD = 4\n",
    "NEURONS_TO_CONSIDER = 50\n",
    "\n",
    "top_neurons_neg_occ = list(top_neurons_neg_mean.items())[:NEURONS_TO_CONSIDER]\n",
    "top_neurons_pos_mean = []\n",
    "neg_indicators = []\n",
    "\n",
    "for entry in top_neurons_neg_occ:\n",
    "    neg_indicators.append(entry[0])\n",
    "\n",
    "print(neg_indicators)\n",
    "\n",
    "# run an inference, if hits threshold on any of the neurons classify as bad\n",
    "e = 0\n",
    "neg_classifications = np.zeros(NEG_SET_SIZE)\n",
    "for example in negative_set:\n",
    "    _, cache = model.run_with_cache_with_saes(example, saes=[sae])\n",
    "\n",
    "    # get data from indicator neurons\n",
    "    val = cache[f'blocks.{num_layer}.hook_resid_post.hook_sae_acts_post'][0, -1, :]\n",
    "    \n",
    "    del cache\n",
    "\n",
    "    for critical_neuron_idx in neg_indicators:\n",
    "        if (val[critical_neuron_idx] > THRESHOLD):\n",
    "            neg_classifications[e] = 1\n",
    "            break\n",
    "    \n",
    "    e += 1\n",
    "\n",
    "pos_classifications = np.zeros(POS_SET_SIZE)\n",
    "\n",
    "\n",
    "e = 0\n",
    "for example in positive_set:\n",
    "    _, cache = model.run_with_cache_with_saes(example, saes=[sae])\n",
    "\n",
    "    # get data from indicator neurons\n",
    "    val = cache[f'blocks.{num_layer}.hook_resid_post.hook_sae_acts_post'][0, -1, :]\n",
    "\n",
    "    del cache\n",
    "    \n",
    "    for critical_neuron_idx in neg_indicators:\n",
    "        if (val[critical_neuron_idx] > THRESHOLD):\n",
    "            pos_classifications[e] = 1\n",
    "            break\n",
    "    e += 1\n",
    "\n",
    "\n",
    "print(\" RATS ! \", np.sum(neg_classifications) / (NEG_SET_SIZE / 100))\n",
    "print(\" GENIUSES ! \", np.sum(pos_classifications) / (NEG_SET_SIZE / 100))\n",
    "\n",
    "with open('run_log.txt', 'a') as file:\n",
    "    file.write(f'SAE Thresholding true negative rate on pos={pos_dataset} neg={neg_dataset} layer={num_layer} dataset_size={NEG_SET_SIZE}: {np.sum(neg_classifications) / (NEG_SET_SIZE / 100)}; false negative rate: {np.sum(pos_classifications) / (NEG_SET_SIZE / 100)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "\n",
    "html_template = \"https://neuronpedia.org/{}/{}/{}?embed=true&embedexplanation=true&embedplots=true&embedtest=true&height=300\"\n",
    "\n",
    "def get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"10-gemmascope-res-16k\", feature_idx=0):\n",
    "    return html_template.format(sae_release, sae_id, feature_idx)\n",
    "\n",
    "for feature_idx in neg_indicators:\n",
    "    html = get_dashboard_html(sae_release = \"gemma-2-2b\", sae_id=\"10-gemmascope-res-16k\", feature_idx=feature_idx)\n",
    "    frame = IFrame(html, width=800, height=400)\n",
    "    display(frame)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
