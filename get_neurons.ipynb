{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "import transformer_lens.utils as utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/cole/.cache/huggingface/token\n",
      "Login successful\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "with open(\"/Users/cole/cs2822r/saes2822r/access.tok\", \"r\") as file:\n",
    "    access_token = file.read()\n",
    "    login(token=access_token)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x1aa094d10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset  \n",
    "import transformer_lens\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/sae_lens/sae.py:136: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load gpt-2-small (for testing)\n",
    "model = HookedSAETransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "# load sae on res stream of gpt-2-small, plus cfg and sparsity val (for testing)\n",
    "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "    release = \"gpt2-small-res-jb\",\n",
    "    sae_id = \"blocks.7.hook_resid_pre\",\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/harmful_strings.csv')\n",
    "\n",
    "columns_as_arrays = [df[col].values for col in df.columns]\n",
    "\n",
    "array_dict = {col: df[col].values for col in df.columns}\n",
    "\n",
    "negative_set = columns_as_arrays[0]\n",
    "negative_set = negative_set[:100]\n",
    "print(len(negative_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "positive = pd.read_json('dataset/alpaca_data.json')\n",
    "\n",
    "positive_set = positive['output'].values\n",
    "positive_set = positive_set[:100]\n",
    "print(len(positive_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae.use_error_term\n",
    "\n",
    "top_neurons_neg = {}\n",
    "top_neurons_pos = {}\n",
    "\n",
    "for example in negative_set:\n",
    "    _, cache = model.run_with_cache_with_saes(example, saes=[sae])\n",
    "\n",
    "    # get top 15 firing sae neurons\n",
    "    vals, inds = torch.topk(cache['blocks.7.hook_resid_pre.hook_sae_acts_post'][0, -1, :], 15)\n",
    "\n",
    "    for datapoint in zip(inds, vals):\n",
    "        if datapoint[0] in top_neurons_neg:\n",
    "            top_neurons_neg[datapoint[0]].append(datapoint[1])\n",
    "        else:\n",
    "            top_neurons_neg[datapoint[0]] = [datapoint[1]]\n",
    "    \n",
    "\n",
    "for example in positive_set:\n",
    "    _, cache = model.run_with_cache_with_saes(example, saes=[sae])\n",
    "\n",
    "    # get top 15 firing sae neurons\n",
    "    vals, inds = torch.topk(cache['blocks.7.hook_resid_pre.hook_sae_acts_post'][0, -1, :], 15)\n",
    "\n",
    "    for datapoint in zip(inds, vals):\n",
    "        if datapoint[0] in top_neurons_pos:\n",
    "            top_neurons_pos[datapoint[0]].append(datapoint[1])\n",
    "        else:\n",
    "            top_neurons_pos[datapoint[0]] = [datapoint[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n",
      "(24576,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     labels_list\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example_txt \u001b[38;5;129;01min\u001b[39;00m positive_set:\n\u001b[0;32m---> 15\u001b[0m     _, cache \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache_with_saes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_txt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msae\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     activations \u001b[38;5;241m=\u001b[39m cache[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblocks.7.hook_resid_pre.hook_sae_acts_post\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     18\u001b[0m     activations_list\u001b[38;5;241m.\u001b[39mappend(activations)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sae_lens/analysis/hooked_sae_transformer.py:225\u001b[0m, in \u001b[0;36mHookedSAETransformer.run_with_cache_with_saes\u001b[0;34m(self, saes, reset_saes_end, use_error_term, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_cache_with_saes\u001b[39m(\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;241m*\u001b[39mmodel_args: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m     Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m    208\u001b[0m ]:\n\u001b[1;32m    209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around 'run_with_cache' in HookedTransformer.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m    Attaches given SAEs before running the model with cache and then removes them.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m        **kwargs: Keyword arguments for the model forward pass\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43msaes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msaes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_saes_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_saes_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_error_term\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_error_term\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    229\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_cache_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_cache_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    231\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.5/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:144\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sae_lens/analysis/hooked_sae_transformer.py:314\u001b[0m, in \u001b[0;36mHookedSAETransformer.saes\u001b[0;34m(self, saes, reset_saes_end, use_error_term)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reset_saes_end:\n\u001b[0;32m--> 314\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_saes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mact_names_to_reset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_saes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sae_lens/analysis/hooked_sae_transformer.py:158\u001b[0m, in \u001b[0;36mHookedSAETransformer.reset_saes\u001b[0;34m(self, act_names, prev_saes)\u001b[0m\n\u001b[1;32m    155\u001b[0m     prev_saes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(act_names)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m act_name, prev_sae \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(act_names, prev_saes):  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset_sae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mact_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_sae\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sae_lens/analysis/hooked_sae_transformer.py:128\u001b[0m, in \u001b[0;36mHookedSAETransformer._reset_sae\u001b[0;34m(self, act_name, prev_sae)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macts_to_saes[act_name] \u001b[38;5;241m=\u001b[39m prev_sae\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m     \u001b[43mset_deep_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHookPoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macts_to_saes[act_name]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/sae_lens/analysis/hooked_sae_transformer.py:51\u001b[0m, in \u001b[0;36mset_deep_attr\u001b[0;34m(obj, path, value)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Navigate to the last component in the path\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m parts[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m part\u001b[38;5;241m.\u001b[39misdigit():  \u001b[38;5;66;03m# This is a list index\u001b[39;00m\n\u001b[1;32m     52\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj[\u001b[38;5;28mint\u001b[39m(part)]\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# This is an attribute\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train classifier on sae activations\n",
    "activations_list = []\n",
    "labels_list = []\n",
    "\n",
    "# 0 = negative, 1 = positive\n",
    "for example_txt in negative_set:\n",
    "    _, cache = model.run_with_cache_with_saes(example_txt, saes=[sae])\n",
    "    activations = cache['blocks.7.hook_resid_pre.hook_sae_acts_post'][0, -1, :].cpu().numpy()\n",
    "    print(activations.shape)\n",
    "\n",
    "    activations_list.append(activations)\n",
    "    labels_list.append(0)\n",
    "\n",
    "for example_txt in positive_set:\n",
    "    _, cache = model.run_with_cache_with_saes(example_txt, saes=[sae])\n",
    "    activations = cache['blocks.7.hook_resid_pre.hook_sae_acts_post'][0, -1, :].cpu().numpy()\n",
    "\n",
    "    activations_list.append(activations)\n",
    "    labels_list.append(1)   \n",
    "\n",
    "# data\n",
    "X = np.array(activations_list)\n",
    "y = np.array(labels_list)\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scale activation features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, solver='lbfgs') \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "(768,)\n",
      "Test Accuracy: 0.8500\n"
     ]
    }
   ],
   "source": [
    "# train classifier on base activations\n",
    "activations_list = []\n",
    "labels_list = []\n",
    "\n",
    "# 0 = negative, 1 = positive\n",
    "for example_txt in negative_set:\n",
    "    _, cache = model.run_with_cache(example_txt)\n",
    "    res_stream = cache.decompose_resid(layer=8, return_labels=False, mode='attn', incl_embeds=False, pos_slice=slice(0, -1))\n",
    "    seven_out = res_stream[-1, 0, -1, :].cpu().numpy()  # layer batch pos d_model\n",
    "    print(seven_out.shape)\n",
    "\n",
    "    activations_list.append(seven_out)\n",
    "    labels_list.append(0)\n",
    "\n",
    "for example_txt in positive_set:\n",
    "    _, cache = model.run_with_cache(example_txt)\n",
    "    res_stream = cache.decompose_resid(layer=8, return_labels=False, mode='attn', incl_embeds=False, pos_slice=slice(0, -1))\n",
    "    seven_out = res_stream[-1, 0, -1, :].cpu().numpy()  # layer batch pos d_model\n",
    "\n",
    "    activations_list.append(seven_out)\n",
    "    labels_list.append(1)\n",
    "\n",
    "# data and split\n",
    "X = np.array(activations_list)\n",
    "y = np.array(labels_list)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scale features \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# train classifier\n",
    "clf = LogisticRegression(max_iter=1000, solver='lbfgs') \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reject sample if activation at any of top N neurons is above threshold T\n",
    "THRESHOLD = 2\n",
    "NEURONS_TO_CONSIDER = 10\n",
    "\n",
    "top_neurons_neg_mean = [(8023, 41), (6056, 23), (3261, 23), (6136, 21), (21916, 18), (11946, 17), (19967, 16), (17723, 15), (16804, 15), (3087, 14), (13368, 14), (5481, 12), (14325, 12), (22930, 11), (8011, 11), (1460, 10), (14179, 10), (14867, 10), (8244, 10), (20483, 9), (22309, 9), (5298, 8), (17151, 8), (15637, 8), (19092, 8), (14687, 8), (19351, 8), (24391, 7), (20960, 7), (720, 7), (16672, 7), (3540, 7), (7186, 7), (12163, 7), (2226, 7), (13306, 7), (6647, 7), (20936, 7), (6670, 6), (3045, 6), (9307, 6), (133, 6), (22721, 6), (21485, 6), (513, 6), (17732, 6), (1911, 6), (2831, 6), (11712, 6), (11213, 6), (15557, 6), (3092, 6), (21040, 6), (2284, 6), (20658, 6), (16694, 6), (22822, 6), (2312, 5), (17282, 5), (22894, 5), (15423, 5), (6302, 5), (4808, 5), (303, 5), (2908, 5), (20511, 5), (5834, 5), (307, 5), (14490, 5), (16038, 5), (4478, 5), (14743, 5), (6526, 5), (4495, 5), (4415, 5), (6524, 5), (7323, 5), (14633, 5), (3578, 5), (15102, 5), (14929, 5), (20157, 5), (22815, 5), (20572, 5), (13804, 5), (19045, 4), (127, 4), (3879, 4), (21309, 4), (20505, 4), (17228, 4), (16466, 4), (13702, 4), (3108, 4), (10072, 4), (17112, 4), (22881, 4), (18372, 4), (22414, 4), (12656, 4)]\n",
    "top_neurons_pos_mean = [(2928, 43), (13638, 37), (14812, 37), (9749, 33), (23171, 32), (19991, 31), (3088, 30), (2382, 28), (5226, 27), (20364, 24), (2367, 23), (323, 23), (12588, 22), (23013, 22), (1724, 20), (23676, 20), (355, 19), (23595, 19), (22579, 18), (3290, 18), (10655, 18), (1432, 17), (501, 17), (18243, 17), (10680, 15), (15585, 14), (9976, 14), (19363, 13), (7274, 13), (11299, 12), (23163, 12), (20035, 12), (7046, 12), (23126, 12), (19711, 12), (6849, 12), (15617, 12), (1664, 11), (981, 11), (20365, 11), (19528, 11), (18539, 10), (14993, 10), (15106, 10), (10550, 10), (22595, 10), (24158, 10), (10110, 10), (18986, 10), (12940, 10), (20454, 10), (7085, 10), (11990, 10), (13726, 10), (8872, 9), (18607, 9), (386, 9), (21014, 9), (15955, 9), (3406, 9), (14719, 9), (4447, 9), (13758, 9), (6726, 8), (14024, 8), (10845, 8), (19908, 8), (17213, 8), (18626, 8), (2617, 8), (15580, 8), (11310, 8), (6979, 8), (506, 8), (8095, 8), (9680, 8), (24427, 7), (6610, 7), (14237, 7), (6584, 7), (1125, 7), (18881, 7), (4354, 7), (14390, 7), (13286, 7), (6213, 7), (12377, 7), (3805, 7), (3631, 7), (13505, 7), (22162, 6), (621, 6), (10947, 6), (2891, 6), (21286, 6), (13437, 6), (8715, 6), (13512, 6), (1644, 6), (14913, 6)]\n",
    "neg_indicators = []\n",
    "\n",
    "for entry in top_neurons_neg_mean:\n",
    "    neg_indicators.append(entry[0])\n",
    "\n",
    "print(neg_indicators)\n",
    "\n",
    "# run an inference, if hits threshold on any of the neurons classify as bad\n",
    "e = 0\n",
    "neg_classifications = np.zeros(200)\n",
    "for example in negative_set:\n",
    "    _, cache = model.run_with_cache_with_saes(example, saes=[sae])\n",
    "\n",
    "    # get data from indicator neurons\n",
    "    val = cache['blocks.7.hook_resid_pre.hook_sae_acts_post'][0, -1, :]\n",
    "    for critical_neuron_idx in neg_indicators:\n",
    "        if (val[critical_neuron_idx] > THRESHOLD):\n",
    "            neg_classifications[e] = 1\n",
    "            break\n",
    "    e += 1\n",
    "\n",
    "pos_classifications = np.zeros(200)\n",
    "\n",
    "\n",
    "e = 0\n",
    "for example in positive_set:\n",
    "    _, cache = model.run_with_cache_with_saes(example, saes=[sae])\n",
    "\n",
    "    # get data from indicator neurons\n",
    "    val = cache['blocks.7.hook_resid_pre.hook_sae_acts_post'][0, -1, :]\n",
    "    for critical_neuron_idx in neg_indicators:\n",
    "        if (val[critical_neuron_idx] > THRESHOLD):\n",
    "            pos_classifications[e] = 1\n",
    "            break\n",
    "    e += 1\n",
    "\n",
    "print(\" RATS ! \", np.sum(neg_classifications) / 2)\n",
    "print(\" GENIUSES ! \", np.sum(pos_classifications) / 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
